{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeAvjQGrgf7+qrCYs8WH86",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anfins/Neural-Network/blob/main/Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using a Multi-Layer Perceptron to Predict March Madness 2023\n",
        "Jorden Anfinson\n",
        "\n",
        "\n",
        "Due to its 6-round tournament format with 64 teams, March Madness is notoriously difficult to predict accurately. Typically, forecasting March Madness entails using a model to predict the outcome of each game, in the hopes that the model will predict each one correctly. Building a model capable of doing so is considered next to impossible, and no model to this point has been able to do so. However, a more manageable task is to forecast the round in which each team will be eliminated. In this notebook, we explore the effectiveness of using a multi-layer perceptron to classify college basketball teams based on the round of March Madness in which they are eliminated. In doing so, we create a network that can accurately forecast the final placement of March Madness teams nearly 50 percent of the time, and find that Kansas, USC, Arizona, UCLA and Alabama have the best chance of winning the 2023 NCAA Tournament."
      ],
      "metadata": {
        "id": "MMG3rwYX2z9g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vht-S_sg7mU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import math\n",
        "import cv2\n",
        "import cv2 as cv\n",
        "import pytest\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets   #only for the purposes of loading the iris dataset\n",
        "import tensorflow as t\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "# Set the random seed for numpy\n",
        "np.random.seed(1)\n",
        "\n",
        "# Set the random seed for TensorFlow\n",
        "t.random.set_seed(1)\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from scipy import linalg as la\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Source\n"
      ],
      "metadata": {
        "id": "DHxZCTatm8wP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The data used in this notebook stems from a kaggle dataset hosted at the following URL:  https://www.kaggle.com/datasets/andrewsundberg/college-basketball-dataset\n",
        "\n",
        "It is also available in csv form at: https://raw.githubusercontent.com/anfins/Neural-Network/main/cbb.csv\n",
        "\n",
        "\n",
        "The data in this dataset was originally scraped from the advanced college basketball statistics website \"Barttorvik\". It consists of each college basketball team's stats from 2013 to 2021, and of 24 features in total. Some of the features in the data include the team's name, home conference, resulting tournament seed and the year the team played. It additionally consists of more advanced statistics such as BARTHAG, which is a numeric representation of the percent chance that a given team would beat the average D1 basketball team.\n",
        "\n",
        "We use the data from the 2013-2021 seasons to construct the neural network we  use to predict the final placement of 2023 College Basketball teams in March Madness. This data serves as our training data for the network.\n",
        "\n"
      ],
      "metadata": {
        "id": "DoCJdxcE3UcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/anfins/Neural-Network/main/cbb.csv\" #I stored the data on my github and connected it to the notebook\n",
        "data = pd.read_csv(url, sep = \",\")\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "mZ2-DO0Hj-5P",
        "outputId": "1c9a4731-8b73-4b46-d45d-487c9fbbebce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             TEAM CONF   G   W  ADJOE  ADJDE  BARTHAG  EFG_O  EFG_D   TOR  \\\n",
              "0            Duke  ACC  39  35  125.2   90.6   0.9764   56.6   46.5  16.3   \n",
              "1  North Carolina  ACC  40  33  123.3   94.9   0.9531   52.6   48.1  15.4   \n",
              "2        Syracuse  ACC  37  23  111.9   93.6   0.8857   50.0   47.3  18.1   \n",
              "3      Louisville  ACC  36  27  109.4   87.4   0.9290   47.7   44.0  17.2   \n",
              "4  North Carolina  ACC  38  26  119.6   92.5   0.9507   51.6   45.4  18.2   \n",
              "\n",
              "   ...  FTRD  2P_O  2P_D  3P_O  3P_D  ADJ_T   WAB  POSTSEASON  SEED  YEAR  \n",
              "0  ...  23.9  55.9  46.3  38.7  31.4   66.4  10.7   Champions   1.0  2015  \n",
              "1  ...  30.4  53.9  44.6  32.7  36.2   71.7   8.6         2ND   1.0  2016  \n",
              "2  ...  28.0  47.2  48.1  36.0  30.7   65.5  -0.3          F4  10.0  2016  \n",
              "3  ...  33.3  48.4  43.3  30.7  30.3   65.6   5.8          E8   4.0  2015  \n",
              "4  ...  37.8  50.9  45.6  35.8  30.0   69.9   6.5         S16   4.0  2015  \n",
              "\n",
              "[5 rows x 24 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-153fee0d-3d19-4446-b1a4-303254651370\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TEAM</th>\n",
              "      <th>CONF</th>\n",
              "      <th>G</th>\n",
              "      <th>W</th>\n",
              "      <th>ADJOE</th>\n",
              "      <th>ADJDE</th>\n",
              "      <th>BARTHAG</th>\n",
              "      <th>EFG_O</th>\n",
              "      <th>EFG_D</th>\n",
              "      <th>TOR</th>\n",
              "      <th>...</th>\n",
              "      <th>FTRD</th>\n",
              "      <th>2P_O</th>\n",
              "      <th>2P_D</th>\n",
              "      <th>3P_O</th>\n",
              "      <th>3P_D</th>\n",
              "      <th>ADJ_T</th>\n",
              "      <th>WAB</th>\n",
              "      <th>POSTSEASON</th>\n",
              "      <th>SEED</th>\n",
              "      <th>YEAR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Duke</td>\n",
              "      <td>ACC</td>\n",
              "      <td>39</td>\n",
              "      <td>35</td>\n",
              "      <td>125.2</td>\n",
              "      <td>90.6</td>\n",
              "      <td>0.9764</td>\n",
              "      <td>56.6</td>\n",
              "      <td>46.5</td>\n",
              "      <td>16.3</td>\n",
              "      <td>...</td>\n",
              "      <td>23.9</td>\n",
              "      <td>55.9</td>\n",
              "      <td>46.3</td>\n",
              "      <td>38.7</td>\n",
              "      <td>31.4</td>\n",
              "      <td>66.4</td>\n",
              "      <td>10.7</td>\n",
              "      <td>Champions</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>North Carolina</td>\n",
              "      <td>ACC</td>\n",
              "      <td>40</td>\n",
              "      <td>33</td>\n",
              "      <td>123.3</td>\n",
              "      <td>94.9</td>\n",
              "      <td>0.9531</td>\n",
              "      <td>52.6</td>\n",
              "      <td>48.1</td>\n",
              "      <td>15.4</td>\n",
              "      <td>...</td>\n",
              "      <td>30.4</td>\n",
              "      <td>53.9</td>\n",
              "      <td>44.6</td>\n",
              "      <td>32.7</td>\n",
              "      <td>36.2</td>\n",
              "      <td>71.7</td>\n",
              "      <td>8.6</td>\n",
              "      <td>2ND</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Syracuse</td>\n",
              "      <td>ACC</td>\n",
              "      <td>37</td>\n",
              "      <td>23</td>\n",
              "      <td>111.9</td>\n",
              "      <td>93.6</td>\n",
              "      <td>0.8857</td>\n",
              "      <td>50.0</td>\n",
              "      <td>47.3</td>\n",
              "      <td>18.1</td>\n",
              "      <td>...</td>\n",
              "      <td>28.0</td>\n",
              "      <td>47.2</td>\n",
              "      <td>48.1</td>\n",
              "      <td>36.0</td>\n",
              "      <td>30.7</td>\n",
              "      <td>65.5</td>\n",
              "      <td>-0.3</td>\n",
              "      <td>F4</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Louisville</td>\n",
              "      <td>ACC</td>\n",
              "      <td>36</td>\n",
              "      <td>27</td>\n",
              "      <td>109.4</td>\n",
              "      <td>87.4</td>\n",
              "      <td>0.9290</td>\n",
              "      <td>47.7</td>\n",
              "      <td>44.0</td>\n",
              "      <td>17.2</td>\n",
              "      <td>...</td>\n",
              "      <td>33.3</td>\n",
              "      <td>48.4</td>\n",
              "      <td>43.3</td>\n",
              "      <td>30.7</td>\n",
              "      <td>30.3</td>\n",
              "      <td>65.6</td>\n",
              "      <td>5.8</td>\n",
              "      <td>E8</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>North Carolina</td>\n",
              "      <td>ACC</td>\n",
              "      <td>38</td>\n",
              "      <td>26</td>\n",
              "      <td>119.6</td>\n",
              "      <td>92.5</td>\n",
              "      <td>0.9507</td>\n",
              "      <td>51.6</td>\n",
              "      <td>45.4</td>\n",
              "      <td>18.2</td>\n",
              "      <td>...</td>\n",
              "      <td>37.8</td>\n",
              "      <td>50.9</td>\n",
              "      <td>45.6</td>\n",
              "      <td>35.8</td>\n",
              "      <td>30.0</td>\n",
              "      <td>69.9</td>\n",
              "      <td>6.5</td>\n",
              "      <td>S16</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 24 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-153fee0d-3d19-4446-b1a4-303254651370')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-153fee0d-3d19-4446-b1a4-303254651370 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-153fee0d-3d19-4446-b1a4-303254651370');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the dataset above lacks team data from the 2023 College Basketball season, we read in the data of College Basketball Teams **that made the 2023 March Madness Tournament** separately in the code below. For clarity, this data is referred to as \"2023 Tournament Team Data\" throughout the rest of this notebook. We later feed this data into the network we created by using the team data from 2013-2021 to predict the placement of these teams. This will serve as our final testing set for our network.\n",
        "\n",
        "Note : While this paper was published after the culmination of the 2023 March Madness Touranment (congrats UConn!!), only team statistics from before the start of the tournament were used to construct the data. We also still include the team's seed, as that was assigned prior to the start of the tournament."
      ],
      "metadata": {
        "id": "lhMLbqo6-hhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/anfins/Neural-Network/main/CurrentCBB2.csv\" #I stored the data on my github and connected it to the notebook\n",
        "currentData = pd.read_csv(url, sep = \",\") #reading in the data of teams from the 2023 season\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RPnks3oGS6P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "currentData.head() #note that the POSTSEASON column is left NaN to indicate these teams had not finished their season (still had yet to compete in March Madness)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "IYuooJXwEpXr",
        "outputId": "8156d8d4-3167-40ac-a648-f1bc46c35637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            TEAM  CONF   G   W  ADJOE  ADJDE  BARTHAG  EFG_O  EFG_D   TOR  \\\n",
              "0        Houston  Amer  34  31  117.1   88.0   0.9638   52.7   42.5  15.3   \n",
              "1        Alabama   SEC  34  29  115.4   88.3   0.9557   52.7   41.5  19.0   \n",
              "2      Creighton    BE  33  21  113.7   92.9   0.9112   54.3   47.3  16.6   \n",
              "3    Connecticut    BE  33  25  118.8   92.5   0.9466   53.5   45.5  18.9   \n",
              "4  San Diego St.   MWC  36  31  110.8   88.8   0.9276   49.6   46.3  17.4   \n",
              "\n",
              "   ...  FTRD  2P_O  2P_D  3P_O  3P_D  ADJ_T   WAB  POSTSEASON  SEED    YEAR  \n",
              "0  ...  38.5  53.2  43.1  34.5  27.8   64.0   8.0         NaN   1.0  2023.0  \n",
              "1  ...  32.8  54.6  41.2  33.8  28.1   73.5  10.2         NaN   1.0  2023.0  \n",
              "2  ...  19.8  54.5  45.6  36.0  34.1   69.9  10.0         NaN   6.0  2023.0  \n",
              "3  ...  38.7  53.4  45.8  35.7  30.0   67.7   4.9         NaN   4.0  2023.0  \n",
              "4  ...  29.4  49.0  49.4  33.9  27.8   65.7   5.7         NaN   5.0  2023.0  \n",
              "\n",
              "[5 rows x 24 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9b729c84-5017-4712-856b-697ec5b0584d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TEAM</th>\n",
              "      <th>CONF</th>\n",
              "      <th>G</th>\n",
              "      <th>W</th>\n",
              "      <th>ADJOE</th>\n",
              "      <th>ADJDE</th>\n",
              "      <th>BARTHAG</th>\n",
              "      <th>EFG_O</th>\n",
              "      <th>EFG_D</th>\n",
              "      <th>TOR</th>\n",
              "      <th>...</th>\n",
              "      <th>FTRD</th>\n",
              "      <th>2P_O</th>\n",
              "      <th>2P_D</th>\n",
              "      <th>3P_O</th>\n",
              "      <th>3P_D</th>\n",
              "      <th>ADJ_T</th>\n",
              "      <th>WAB</th>\n",
              "      <th>POSTSEASON</th>\n",
              "      <th>SEED</th>\n",
              "      <th>YEAR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Houston</td>\n",
              "      <td>Amer</td>\n",
              "      <td>34</td>\n",
              "      <td>31</td>\n",
              "      <td>117.1</td>\n",
              "      <td>88.0</td>\n",
              "      <td>0.9638</td>\n",
              "      <td>52.7</td>\n",
              "      <td>42.5</td>\n",
              "      <td>15.3</td>\n",
              "      <td>...</td>\n",
              "      <td>38.5</td>\n",
              "      <td>53.2</td>\n",
              "      <td>43.1</td>\n",
              "      <td>34.5</td>\n",
              "      <td>27.8</td>\n",
              "      <td>64.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2023.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Alabama</td>\n",
              "      <td>SEC</td>\n",
              "      <td>34</td>\n",
              "      <td>29</td>\n",
              "      <td>115.4</td>\n",
              "      <td>88.3</td>\n",
              "      <td>0.9557</td>\n",
              "      <td>52.7</td>\n",
              "      <td>41.5</td>\n",
              "      <td>19.0</td>\n",
              "      <td>...</td>\n",
              "      <td>32.8</td>\n",
              "      <td>54.6</td>\n",
              "      <td>41.2</td>\n",
              "      <td>33.8</td>\n",
              "      <td>28.1</td>\n",
              "      <td>73.5</td>\n",
              "      <td>10.2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2023.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Creighton</td>\n",
              "      <td>BE</td>\n",
              "      <td>33</td>\n",
              "      <td>21</td>\n",
              "      <td>113.7</td>\n",
              "      <td>92.9</td>\n",
              "      <td>0.9112</td>\n",
              "      <td>54.3</td>\n",
              "      <td>47.3</td>\n",
              "      <td>16.6</td>\n",
              "      <td>...</td>\n",
              "      <td>19.8</td>\n",
              "      <td>54.5</td>\n",
              "      <td>45.6</td>\n",
              "      <td>36.0</td>\n",
              "      <td>34.1</td>\n",
              "      <td>69.9</td>\n",
              "      <td>10.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2023.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Connecticut</td>\n",
              "      <td>BE</td>\n",
              "      <td>33</td>\n",
              "      <td>25</td>\n",
              "      <td>118.8</td>\n",
              "      <td>92.5</td>\n",
              "      <td>0.9466</td>\n",
              "      <td>53.5</td>\n",
              "      <td>45.5</td>\n",
              "      <td>18.9</td>\n",
              "      <td>...</td>\n",
              "      <td>38.7</td>\n",
              "      <td>53.4</td>\n",
              "      <td>45.8</td>\n",
              "      <td>35.7</td>\n",
              "      <td>30.0</td>\n",
              "      <td>67.7</td>\n",
              "      <td>4.9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2023.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>San Diego St.</td>\n",
              "      <td>MWC</td>\n",
              "      <td>36</td>\n",
              "      <td>31</td>\n",
              "      <td>110.8</td>\n",
              "      <td>88.8</td>\n",
              "      <td>0.9276</td>\n",
              "      <td>49.6</td>\n",
              "      <td>46.3</td>\n",
              "      <td>17.4</td>\n",
              "      <td>...</td>\n",
              "      <td>29.4</td>\n",
              "      <td>49.0</td>\n",
              "      <td>49.4</td>\n",
              "      <td>33.9</td>\n",
              "      <td>27.8</td>\n",
              "      <td>65.7</td>\n",
              "      <td>5.7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2023.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 24 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9b729c84-5017-4712-856b-697ec5b0584d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9b729c84-5017-4712-856b-697ec5b0584d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9b729c84-5017-4712-856b-697ec5b0584d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then combine the team data from the 2013-2021 seasons with the team data from the 2023 season to make the data cleaning process the same for both groups of data."
      ],
      "metadata": {
        "id": "7RXumF7zE5pG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.concat([data, currentData]).reset_index() #combining the data (2013-2021 and 2023 Tournament Team Data Combined)"
      ],
      "metadata": {
        "id": "66alVhLhUmB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Cleaning\n"
      ],
      "metadata": {
        "id": "Ku0VdNaFXmJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We clean the data by first filtering out teams that did not make the Tournament in their respective season from the training data. We do so because our final testing data only consists of teams we know to be in the 2023 March Madness tournament, so we are not interested in predicting whether or not a team will make the postseason or not (which would be a reason to include teams that did not make the Tournament in the training data)."
      ],
      "metadata": {
        "id": "hqH6K0EpF_3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['POSTSEASON'] = data['POSTSEASON'].fillna(\"NO POSTSEASON\")\n",
        "data= data.loc[(data[\"POSTSEASON\"] != \"NO POSTSEASON\") | (data[\"YEAR\"] == 2023)].reset_index() #we keep the data of 2023 tournament teams in the training data until after normalization\n",
        "data = data.drop([\"level_0\"], axis = 1)"
      ],
      "metadata": {
        "id": "HfuXO-T0t81v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "Fy4o77KDbR9r",
        "outputId": "dac1aa08-7752-4d7e-e385-2238868ba5a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   index            TEAM CONF   G   W  ADJOE  ADJDE  BARTHAG  EFG_O  EFG_D  \\\n",
              "0      0            Duke  ACC  39  35  125.2   90.6   0.9764   56.6   46.5   \n",
              "1      1  North Carolina  ACC  40  33  123.3   94.9   0.9531   52.6   48.1   \n",
              "2      2        Syracuse  ACC  37  23  111.9   93.6   0.8857   50.0   47.3   \n",
              "3      3      Louisville  ACC  36  27  109.4   87.4   0.9290   47.7   44.0   \n",
              "4      4  North Carolina  ACC  38  26  119.6   92.5   0.9507   51.6   45.4   \n",
              "\n",
              "   ...  FTRD  2P_O  2P_D  3P_O  3P_D  ADJ_T   WAB  POSTSEASON  SEED    YEAR  \n",
              "0  ...  23.9  55.9  46.3  38.7  31.4   66.4  10.7   Champions   1.0  2015.0  \n",
              "1  ...  30.4  53.9  44.6  32.7  36.2   71.7   8.6         2ND   1.0  2016.0  \n",
              "2  ...  28.0  47.2  48.1  36.0  30.7   65.5  -0.3          F4  10.0  2016.0  \n",
              "3  ...  33.3  48.4  43.3  30.7  30.3   65.6   5.8          E8   4.0  2015.0  \n",
              "4  ...  37.8  50.9  45.6  35.8  30.0   69.9   6.5         S16   4.0  2015.0  \n",
              "\n",
              "[5 rows x 25 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ef9493fd-8eb9-4852-8387-916d245ac798\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>TEAM</th>\n",
              "      <th>CONF</th>\n",
              "      <th>G</th>\n",
              "      <th>W</th>\n",
              "      <th>ADJOE</th>\n",
              "      <th>ADJDE</th>\n",
              "      <th>BARTHAG</th>\n",
              "      <th>EFG_O</th>\n",
              "      <th>EFG_D</th>\n",
              "      <th>...</th>\n",
              "      <th>FTRD</th>\n",
              "      <th>2P_O</th>\n",
              "      <th>2P_D</th>\n",
              "      <th>3P_O</th>\n",
              "      <th>3P_D</th>\n",
              "      <th>ADJ_T</th>\n",
              "      <th>WAB</th>\n",
              "      <th>POSTSEASON</th>\n",
              "      <th>SEED</th>\n",
              "      <th>YEAR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Duke</td>\n",
              "      <td>ACC</td>\n",
              "      <td>39</td>\n",
              "      <td>35</td>\n",
              "      <td>125.2</td>\n",
              "      <td>90.6</td>\n",
              "      <td>0.9764</td>\n",
              "      <td>56.6</td>\n",
              "      <td>46.5</td>\n",
              "      <td>...</td>\n",
              "      <td>23.9</td>\n",
              "      <td>55.9</td>\n",
              "      <td>46.3</td>\n",
              "      <td>38.7</td>\n",
              "      <td>31.4</td>\n",
              "      <td>66.4</td>\n",
              "      <td>10.7</td>\n",
              "      <td>Champions</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2015.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>North Carolina</td>\n",
              "      <td>ACC</td>\n",
              "      <td>40</td>\n",
              "      <td>33</td>\n",
              "      <td>123.3</td>\n",
              "      <td>94.9</td>\n",
              "      <td>0.9531</td>\n",
              "      <td>52.6</td>\n",
              "      <td>48.1</td>\n",
              "      <td>...</td>\n",
              "      <td>30.4</td>\n",
              "      <td>53.9</td>\n",
              "      <td>44.6</td>\n",
              "      <td>32.7</td>\n",
              "      <td>36.2</td>\n",
              "      <td>71.7</td>\n",
              "      <td>8.6</td>\n",
              "      <td>2ND</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2016.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Syracuse</td>\n",
              "      <td>ACC</td>\n",
              "      <td>37</td>\n",
              "      <td>23</td>\n",
              "      <td>111.9</td>\n",
              "      <td>93.6</td>\n",
              "      <td>0.8857</td>\n",
              "      <td>50.0</td>\n",
              "      <td>47.3</td>\n",
              "      <td>...</td>\n",
              "      <td>28.0</td>\n",
              "      <td>47.2</td>\n",
              "      <td>48.1</td>\n",
              "      <td>36.0</td>\n",
              "      <td>30.7</td>\n",
              "      <td>65.5</td>\n",
              "      <td>-0.3</td>\n",
              "      <td>F4</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2016.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Louisville</td>\n",
              "      <td>ACC</td>\n",
              "      <td>36</td>\n",
              "      <td>27</td>\n",
              "      <td>109.4</td>\n",
              "      <td>87.4</td>\n",
              "      <td>0.9290</td>\n",
              "      <td>47.7</td>\n",
              "      <td>44.0</td>\n",
              "      <td>...</td>\n",
              "      <td>33.3</td>\n",
              "      <td>48.4</td>\n",
              "      <td>43.3</td>\n",
              "      <td>30.7</td>\n",
              "      <td>30.3</td>\n",
              "      <td>65.6</td>\n",
              "      <td>5.8</td>\n",
              "      <td>E8</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2015.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>North Carolina</td>\n",
              "      <td>ACC</td>\n",
              "      <td>38</td>\n",
              "      <td>26</td>\n",
              "      <td>119.6</td>\n",
              "      <td>92.5</td>\n",
              "      <td>0.9507</td>\n",
              "      <td>51.6</td>\n",
              "      <td>45.4</td>\n",
              "      <td>...</td>\n",
              "      <td>37.8</td>\n",
              "      <td>50.9</td>\n",
              "      <td>45.6</td>\n",
              "      <td>35.8</td>\n",
              "      <td>30.0</td>\n",
              "      <td>69.9</td>\n",
              "      <td>6.5</td>\n",
              "      <td>S16</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2015.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 25 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ef9493fd-8eb9-4852-8387-916d245ac798')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ef9493fd-8eb9-4852-8387-916d245ac798 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ef9493fd-8eb9-4852-8387-916d245ac798');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['SEED'] = data['SEED'].fillna(0)\n"
      ],
      "metadata": {
        "id": "yugbzbIYw6px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then convert the columns \"POSTSEASON\" and \"CONF\" into numeric data so that they can be fed through the neural network."
      ],
      "metadata": {
        "id": "qtXtJKtVxOBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"POSTSEASON\"] = data[\"POSTSEASON\"].astype(str) #converting to string"
      ],
      "metadata": {
        "id": "UNT_chNSJpM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"PostseasonNums\"] = 0  #the code below converts the values in the \"POSTSEASON\" column, to numeric values in the \"PostseasonNums\" column\n",
        "for row,rowSeries in data.iterrows():\n",
        "\n",
        "  if(rowSeries[\"POSTSEASON\"] == \"Champions\"):\n",
        "    data.at[row, \"PostseasonNums\"] = 0\n",
        "\n",
        "  elif(rowSeries[\"POSTSEASON\"] == \"2ND\"):\n",
        "    data.at[row, \"PostseasonNums\"] = 1\n",
        "\n",
        "  elif(rowSeries[\"POSTSEASON\"] == \"F4\"):\n",
        "    data.at[row, \"PostseasonNums\"] = 2\n",
        "\n",
        "  elif(rowSeries[\"POSTSEASON\"] == \"E8\"):\n",
        "    data.at[row, \"PostseasonNums\"] = 3\n",
        "\n",
        "  elif(rowSeries[\"POSTSEASON\"] == \"S16\"):\n",
        "    data.at[row, \"PostseasonNums\"] = 4\n",
        "\n",
        "  elif(rowSeries[\"POSTSEASON\"] == \"R32\"):\n",
        "    data.at[row, \"PostseasonNums\"] = 5\n",
        "\n",
        "  elif(rowSeries[\"POSTSEASON\"] == \"R64\"):\n",
        "    data.at[row, \"PostseasonNums\"] = 6\n",
        "\n",
        "  elif(rowSeries[\"POSTSEASON\"] == \"R68\"):\n",
        "    data.at[row, \"PostseasonNums\"] = 7\n",
        "  else:\n",
        "    data.at[row, \"PostseasonNums\"] = 8\n"
      ],
      "metadata": {
        "id": "k1vRf3qTIh7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classNames = [\"Champions\", \"Runners Up\", \"Final 4\", \"Elite 8\", \"Sweet 16\", \"Round of 32\", \"Round of 64\", \"Round of 68\", \"NA\"]"
      ],
      "metadata": {
        "id": "x_Yqv1KDUsqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"CONF\"] = pd.Categorical(data[\"CONF\"])\n",
        "data[\"CONFB\"] = data[\"CONF\"].cat.codes"
      ],
      "metadata": {
        "id": "AQkNdkLfxSLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop([\"index\"], axis = 1)"
      ],
      "metadata": {
        "id": "tlStdWTgXqJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result of these changes can be seen below."
      ],
      "metadata": {
        "id": "C7JHQ0vSIuCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "dOh_iOa8av4a",
        "outputId": "d686eeec-c29c-45dc-bb6b-1619bbbb1ffa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       TEAM  CONF   G   W  ADJOE  ADJDE  BARTHAG  EFG_O  \\\n",
              "0                      Duke   ACC  39  35  125.2   90.6   0.9764   56.6   \n",
              "1            North Carolina   ACC  40  33  123.3   94.9   0.9531   52.6   \n",
              "2                  Syracuse   ACC  37  23  111.9   93.6   0.8857   50.0   \n",
              "3                Louisville   ACC  36  27  109.4   87.4   0.9290   47.7   \n",
              "4            North Carolina   ACC  38  26  119.6   92.5   0.9507   51.6   \n",
              "..                      ...   ...  ..  ..    ...    ...      ...    ...   \n",
              "542           UNC Asheville  Bsth  32  25  101.2  102.4   0.4668   53.9   \n",
              "543  Texas A&M Corpus Chris  SInd  30  20  104.3  106.0   0.4540   50.9   \n",
              "544                 Howard   MEAC  31  19  100.9  105.7   0.3695   52.0   \n",
              "545  Southeast Missouri St.   OVC  34  17   98.6  106.8   0.2856   50.1   \n",
              "546     Fairleigh Dickinson   NEC  33  18  105.0  116.4   0.2330   51.7   \n",
              "\n",
              "     EFG_D   TOR  ...  2P_D  3P_O  3P_D  ADJ_T   WAB     POSTSEASON  SEED  \\\n",
              "0     46.5  16.3  ...  46.3  38.7  31.4   66.4  10.7      Champions   1.0   \n",
              "1     48.1  15.4  ...  44.6  32.7  36.2   71.7   8.6            2ND   1.0   \n",
              "2     47.3  18.1  ...  48.1  36.0  30.7   65.5  -0.3             F4  10.0   \n",
              "3     44.0  17.2  ...  43.3  30.7  30.3   65.6   5.8             E8   4.0   \n",
              "4     45.4  18.2  ...  45.6  35.8  30.0   69.9   6.5            S16   4.0   \n",
              "..     ...   ...  ...   ...   ...   ...    ...   ...            ...   ...   \n",
              "542   48.0  20.0  ...  49.1  38.8  30.5   69.1  -1.9  NO POSTSEASON  16.0   \n",
              "543   52.4  18.0  ...  54.3  36.5  33.2   69.8  -6.0  NO POSTSEASON  16.0   \n",
              "544   50.6  22.8  ...  50.8  37.2  33.6   69.7  -7.3  NO POSTSEASON  16.0   \n",
              "545   51.3  17.3  ...  50.7  33.2  35.0   72.8 -11.7  NO POSTSEASON  16.0   \n",
              "546   55.4  16.4  ...  56.1  34.7  36.2   69.4 -12.2  NO POSTSEASON  16.0   \n",
              "\n",
              "       YEAR  PostseasonNums  CONFB  \n",
              "0    2015.0               0      1  \n",
              "1    2016.0               1      1  \n",
              "2    2016.0               2      1  \n",
              "3    2015.0               3      1  \n",
              "4    2015.0               4      1  \n",
              "..      ...             ...    ...  \n",
              "542  2023.0               8     14  \n",
              "543  2023.0               8     31  \n",
              "544  2023.0               8     21  \n",
              "545  2023.0               8     25  \n",
              "546  2023.0               8     24  \n",
              "\n",
              "[547 rows x 26 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-16211cdc-9214-49a1-af14-e07eae7214d6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TEAM</th>\n",
              "      <th>CONF</th>\n",
              "      <th>G</th>\n",
              "      <th>W</th>\n",
              "      <th>ADJOE</th>\n",
              "      <th>ADJDE</th>\n",
              "      <th>BARTHAG</th>\n",
              "      <th>EFG_O</th>\n",
              "      <th>EFG_D</th>\n",
              "      <th>TOR</th>\n",
              "      <th>...</th>\n",
              "      <th>2P_D</th>\n",
              "      <th>3P_O</th>\n",
              "      <th>3P_D</th>\n",
              "      <th>ADJ_T</th>\n",
              "      <th>WAB</th>\n",
              "      <th>POSTSEASON</th>\n",
              "      <th>SEED</th>\n",
              "      <th>YEAR</th>\n",
              "      <th>PostseasonNums</th>\n",
              "      <th>CONFB</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Duke</td>\n",
              "      <td>ACC</td>\n",
              "      <td>39</td>\n",
              "      <td>35</td>\n",
              "      <td>125.2</td>\n",
              "      <td>90.6</td>\n",
              "      <td>0.9764</td>\n",
              "      <td>56.6</td>\n",
              "      <td>46.5</td>\n",
              "      <td>16.3</td>\n",
              "      <td>...</td>\n",
              "      <td>46.3</td>\n",
              "      <td>38.7</td>\n",
              "      <td>31.4</td>\n",
              "      <td>66.4</td>\n",
              "      <td>10.7</td>\n",
              "      <td>Champions</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2015.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>North Carolina</td>\n",
              "      <td>ACC</td>\n",
              "      <td>40</td>\n",
              "      <td>33</td>\n",
              "      <td>123.3</td>\n",
              "      <td>94.9</td>\n",
              "      <td>0.9531</td>\n",
              "      <td>52.6</td>\n",
              "      <td>48.1</td>\n",
              "      <td>15.4</td>\n",
              "      <td>...</td>\n",
              "      <td>44.6</td>\n",
              "      <td>32.7</td>\n",
              "      <td>36.2</td>\n",
              "      <td>71.7</td>\n",
              "      <td>8.6</td>\n",
              "      <td>2ND</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2016.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Syracuse</td>\n",
              "      <td>ACC</td>\n",
              "      <td>37</td>\n",
              "      <td>23</td>\n",
              "      <td>111.9</td>\n",
              "      <td>93.6</td>\n",
              "      <td>0.8857</td>\n",
              "      <td>50.0</td>\n",
              "      <td>47.3</td>\n",
              "      <td>18.1</td>\n",
              "      <td>...</td>\n",
              "      <td>48.1</td>\n",
              "      <td>36.0</td>\n",
              "      <td>30.7</td>\n",
              "      <td>65.5</td>\n",
              "      <td>-0.3</td>\n",
              "      <td>F4</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2016.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Louisville</td>\n",
              "      <td>ACC</td>\n",
              "      <td>36</td>\n",
              "      <td>27</td>\n",
              "      <td>109.4</td>\n",
              "      <td>87.4</td>\n",
              "      <td>0.9290</td>\n",
              "      <td>47.7</td>\n",
              "      <td>44.0</td>\n",
              "      <td>17.2</td>\n",
              "      <td>...</td>\n",
              "      <td>43.3</td>\n",
              "      <td>30.7</td>\n",
              "      <td>30.3</td>\n",
              "      <td>65.6</td>\n",
              "      <td>5.8</td>\n",
              "      <td>E8</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2015.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>North Carolina</td>\n",
              "      <td>ACC</td>\n",
              "      <td>38</td>\n",
              "      <td>26</td>\n",
              "      <td>119.6</td>\n",
              "      <td>92.5</td>\n",
              "      <td>0.9507</td>\n",
              "      <td>51.6</td>\n",
              "      <td>45.4</td>\n",
              "      <td>18.2</td>\n",
              "      <td>...</td>\n",
              "      <td>45.6</td>\n",
              "      <td>35.8</td>\n",
              "      <td>30.0</td>\n",
              "      <td>69.9</td>\n",
              "      <td>6.5</td>\n",
              "      <td>S16</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2015.0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>542</th>\n",
              "      <td>UNC Asheville</td>\n",
              "      <td>Bsth</td>\n",
              "      <td>32</td>\n",
              "      <td>25</td>\n",
              "      <td>101.2</td>\n",
              "      <td>102.4</td>\n",
              "      <td>0.4668</td>\n",
              "      <td>53.9</td>\n",
              "      <td>48.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>49.1</td>\n",
              "      <td>38.8</td>\n",
              "      <td>30.5</td>\n",
              "      <td>69.1</td>\n",
              "      <td>-1.9</td>\n",
              "      <td>NO POSTSEASON</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2023.0</td>\n",
              "      <td>8</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>543</th>\n",
              "      <td>Texas A&amp;M Corpus Chris</td>\n",
              "      <td>SInd</td>\n",
              "      <td>30</td>\n",
              "      <td>20</td>\n",
              "      <td>104.3</td>\n",
              "      <td>106.0</td>\n",
              "      <td>0.4540</td>\n",
              "      <td>50.9</td>\n",
              "      <td>52.4</td>\n",
              "      <td>18.0</td>\n",
              "      <td>...</td>\n",
              "      <td>54.3</td>\n",
              "      <td>36.5</td>\n",
              "      <td>33.2</td>\n",
              "      <td>69.8</td>\n",
              "      <td>-6.0</td>\n",
              "      <td>NO POSTSEASON</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2023.0</td>\n",
              "      <td>8</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>544</th>\n",
              "      <td>Howard</td>\n",
              "      <td>MEAC</td>\n",
              "      <td>31</td>\n",
              "      <td>19</td>\n",
              "      <td>100.9</td>\n",
              "      <td>105.7</td>\n",
              "      <td>0.3695</td>\n",
              "      <td>52.0</td>\n",
              "      <td>50.6</td>\n",
              "      <td>22.8</td>\n",
              "      <td>...</td>\n",
              "      <td>50.8</td>\n",
              "      <td>37.2</td>\n",
              "      <td>33.6</td>\n",
              "      <td>69.7</td>\n",
              "      <td>-7.3</td>\n",
              "      <td>NO POSTSEASON</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2023.0</td>\n",
              "      <td>8</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>545</th>\n",
              "      <td>Southeast Missouri St.</td>\n",
              "      <td>OVC</td>\n",
              "      <td>34</td>\n",
              "      <td>17</td>\n",
              "      <td>98.6</td>\n",
              "      <td>106.8</td>\n",
              "      <td>0.2856</td>\n",
              "      <td>50.1</td>\n",
              "      <td>51.3</td>\n",
              "      <td>17.3</td>\n",
              "      <td>...</td>\n",
              "      <td>50.7</td>\n",
              "      <td>33.2</td>\n",
              "      <td>35.0</td>\n",
              "      <td>72.8</td>\n",
              "      <td>-11.7</td>\n",
              "      <td>NO POSTSEASON</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2023.0</td>\n",
              "      <td>8</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>546</th>\n",
              "      <td>Fairleigh Dickinson</td>\n",
              "      <td>NEC</td>\n",
              "      <td>33</td>\n",
              "      <td>18</td>\n",
              "      <td>105.0</td>\n",
              "      <td>116.4</td>\n",
              "      <td>0.2330</td>\n",
              "      <td>51.7</td>\n",
              "      <td>55.4</td>\n",
              "      <td>16.4</td>\n",
              "      <td>...</td>\n",
              "      <td>56.1</td>\n",
              "      <td>34.7</td>\n",
              "      <td>36.2</td>\n",
              "      <td>69.4</td>\n",
              "      <td>-12.2</td>\n",
              "      <td>NO POSTSEASON</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2023.0</td>\n",
              "      <td>8</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>547 rows × 26 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-16211cdc-9214-49a1-af14-e07eae7214d6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-16211cdc-9214-49a1-af14-e07eae7214d6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-16211cdc-9214-49a1-af14-e07eae7214d6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then reseparate the data, 2023 Tournament Teams (final testing data) go into the dataframe \"currentTournamentTeams\" and 2013-2021 Tournament teams go into the dataframe \"data\". We do so because we must use the mean and standard deviation of our training set to normalize our testing data, and in order to calculate those values our training set must be kept separate from our final testing data."
      ],
      "metadata": {
        "id": "isio4p43Nj0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "currentTournamentTeams = data[data[\"YEAR\"] == 2023].reset_index()\n",
        "data = data[data[\"YEAR\"] != 2023].reset_index()"
      ],
      "metadata": {
        "id": "6we4y5D4MO6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "currentTournamentTeams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "tx8xnaXpPf56",
        "outputId": "e01a626b-c358-4992-d585-73f379f3eab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    index                    TEAM  CONF   G   W  ADJOE  ADJDE  BARTHAG  EFG_O  \\\n",
              "0     480                 Houston  Amer  34  31  117.1   88.0   0.9638   52.7   \n",
              "1     481                 Alabama   SEC  34  29  115.4   88.3   0.9557   52.7   \n",
              "2     482               Creighton    BE  33  21  113.7   92.9   0.9112   54.3   \n",
              "3     483             Connecticut    BE  33  25  118.8   92.5   0.9466   53.5   \n",
              "4     484           San Diego St.   MWC  36  31  110.8   88.8   0.9276   49.6   \n",
              "..    ...                     ...   ...  ..  ..    ...    ...      ...    ...   \n",
              "62    542           UNC Asheville  Bsth  32  25  101.2  102.4   0.4668   53.9   \n",
              "63    543  Texas A&M Corpus Chris  SInd  30  20  104.3  106.0   0.4540   50.9   \n",
              "64    544                 Howard   MEAC  31  19  100.9  105.7   0.3695   52.0   \n",
              "65    545  Southeast Missouri St.   OVC  34  17   98.6  106.8   0.2856   50.1   \n",
              "66    546     Fairleigh Dickinson   NEC  33  18  105.0  116.4   0.2330   51.7   \n",
              "\n",
              "    EFG_D  ...  2P_D  3P_O  3P_D  ADJ_T   WAB     POSTSEASON  SEED    YEAR  \\\n",
              "0    42.5  ...  43.1  34.5  27.8   64.0   8.0  NO POSTSEASON   1.0  2023.0   \n",
              "1    41.5  ...  41.2  33.8  28.1   73.5  10.2  NO POSTSEASON   1.0  2023.0   \n",
              "2    47.3  ...  45.6  36.0  34.1   69.9  10.0  NO POSTSEASON   6.0  2023.0   \n",
              "3    45.5  ...  45.8  35.7  30.0   67.7   4.9  NO POSTSEASON   4.0  2023.0   \n",
              "4    46.3  ...  49.4  33.9  27.8   65.7   5.7  NO POSTSEASON   5.0  2023.0   \n",
              "..    ...  ...   ...   ...   ...    ...   ...            ...   ...     ...   \n",
              "62   48.0  ...  49.1  38.8  30.5   69.1  -1.9  NO POSTSEASON  16.0  2023.0   \n",
              "63   52.4  ...  54.3  36.5  33.2   69.8  -6.0  NO POSTSEASON  16.0  2023.0   \n",
              "64   50.6  ...  50.8  37.2  33.6   69.7  -7.3  NO POSTSEASON  16.0  2023.0   \n",
              "65   51.3  ...  50.7  33.2  35.0   72.8 -11.7  NO POSTSEASON  16.0  2023.0   \n",
              "66   55.4  ...  56.1  34.7  36.2   69.4 -12.2  NO POSTSEASON  16.0  2023.0   \n",
              "\n",
              "    PostseasonNums  CONFB  \n",
              "0                8      4  \n",
              "1                8     30  \n",
              "2                8      9  \n",
              "3                8      9  \n",
              "4                8     23  \n",
              "..             ...    ...  \n",
              "62               8     14  \n",
              "63               8     31  \n",
              "64               8     21  \n",
              "65               8     25  \n",
              "66               8     24  \n",
              "\n",
              "[67 rows x 27 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-355de02f-6c23-4306-92c4-0fbc7e05e658\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>TEAM</th>\n",
              "      <th>CONF</th>\n",
              "      <th>G</th>\n",
              "      <th>W</th>\n",
              "      <th>ADJOE</th>\n",
              "      <th>ADJDE</th>\n",
              "      <th>BARTHAG</th>\n",
              "      <th>EFG_O</th>\n",
              "      <th>EFG_D</th>\n",
              "      <th>...</th>\n",
              "      <th>2P_D</th>\n",
              "      <th>3P_O</th>\n",
              "      <th>3P_D</th>\n",
              "      <th>ADJ_T</th>\n",
              "      <th>WAB</th>\n",
              "      <th>POSTSEASON</th>\n",
              "      <th>SEED</th>\n",
              "      <th>YEAR</th>\n",
              "      <th>PostseasonNums</th>\n",
              "      <th>CONFB</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>480</td>\n",
              "      <td>Houston</td>\n",
              "      <td>Amer</td>\n",
              "      <td>34</td>\n",
              "      <td>31</td>\n",
              "      <td>117.1</td>\n",
              "      <td>88.0</td>\n",
              "      <td>0.9638</td>\n",
              "      <td>52.7</td>\n",
              "      <td>42.5</td>\n",
              "      <td>...</td>\n",
              "      <td>43.1</td>\n",
              "      <td>34.5</td>\n",
              "      <td>27.8</td>\n",
              "      <td>64.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>NO POSTSEASON</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2023.0</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>481</td>\n",
              "      <td>Alabama</td>\n",
              "      <td>SEC</td>\n",
              "      <td>34</td>\n",
              "      <td>29</td>\n",
              "      <td>115.4</td>\n",
              "      <td>88.3</td>\n",
              "      <td>0.9557</td>\n",
              "      <td>52.7</td>\n",
              "      <td>41.5</td>\n",
              "      <td>...</td>\n",
              "      <td>41.2</td>\n",
              "      <td>33.8</td>\n",
              "      <td>28.1</td>\n",
              "      <td>73.5</td>\n",
              "      <td>10.2</td>\n",
              "      <td>NO POSTSEASON</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2023.0</td>\n",
              "      <td>8</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>482</td>\n",
              "      <td>Creighton</td>\n",
              "      <td>BE</td>\n",
              "      <td>33</td>\n",
              "      <td>21</td>\n",
              "      <td>113.7</td>\n",
              "      <td>92.9</td>\n",
              "      <td>0.9112</td>\n",
              "      <td>54.3</td>\n",
              "      <td>47.3</td>\n",
              "      <td>...</td>\n",
              "      <td>45.6</td>\n",
              "      <td>36.0</td>\n",
              "      <td>34.1</td>\n",
              "      <td>69.9</td>\n",
              "      <td>10.0</td>\n",
              "      <td>NO POSTSEASON</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2023.0</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>483</td>\n",
              "      <td>Connecticut</td>\n",
              "      <td>BE</td>\n",
              "      <td>33</td>\n",
              "      <td>25</td>\n",
              "      <td>118.8</td>\n",
              "      <td>92.5</td>\n",
              "      <td>0.9466</td>\n",
              "      <td>53.5</td>\n",
              "      <td>45.5</td>\n",
              "      <td>...</td>\n",
              "      <td>45.8</td>\n",
              "      <td>35.7</td>\n",
              "      <td>30.0</td>\n",
              "      <td>67.7</td>\n",
              "      <td>4.9</td>\n",
              "      <td>NO POSTSEASON</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2023.0</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>484</td>\n",
              "      <td>San Diego St.</td>\n",
              "      <td>MWC</td>\n",
              "      <td>36</td>\n",
              "      <td>31</td>\n",
              "      <td>110.8</td>\n",
              "      <td>88.8</td>\n",
              "      <td>0.9276</td>\n",
              "      <td>49.6</td>\n",
              "      <td>46.3</td>\n",
              "      <td>...</td>\n",
              "      <td>49.4</td>\n",
              "      <td>33.9</td>\n",
              "      <td>27.8</td>\n",
              "      <td>65.7</td>\n",
              "      <td>5.7</td>\n",
              "      <td>NO POSTSEASON</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2023.0</td>\n",
              "      <td>8</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>542</td>\n",
              "      <td>UNC Asheville</td>\n",
              "      <td>Bsth</td>\n",
              "      <td>32</td>\n",
              "      <td>25</td>\n",
              "      <td>101.2</td>\n",
              "      <td>102.4</td>\n",
              "      <td>0.4668</td>\n",
              "      <td>53.9</td>\n",
              "      <td>48.0</td>\n",
              "      <td>...</td>\n",
              "      <td>49.1</td>\n",
              "      <td>38.8</td>\n",
              "      <td>30.5</td>\n",
              "      <td>69.1</td>\n",
              "      <td>-1.9</td>\n",
              "      <td>NO POSTSEASON</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2023.0</td>\n",
              "      <td>8</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>543</td>\n",
              "      <td>Texas A&amp;M Corpus Chris</td>\n",
              "      <td>SInd</td>\n",
              "      <td>30</td>\n",
              "      <td>20</td>\n",
              "      <td>104.3</td>\n",
              "      <td>106.0</td>\n",
              "      <td>0.4540</td>\n",
              "      <td>50.9</td>\n",
              "      <td>52.4</td>\n",
              "      <td>...</td>\n",
              "      <td>54.3</td>\n",
              "      <td>36.5</td>\n",
              "      <td>33.2</td>\n",
              "      <td>69.8</td>\n",
              "      <td>-6.0</td>\n",
              "      <td>NO POSTSEASON</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2023.0</td>\n",
              "      <td>8</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>544</td>\n",
              "      <td>Howard</td>\n",
              "      <td>MEAC</td>\n",
              "      <td>31</td>\n",
              "      <td>19</td>\n",
              "      <td>100.9</td>\n",
              "      <td>105.7</td>\n",
              "      <td>0.3695</td>\n",
              "      <td>52.0</td>\n",
              "      <td>50.6</td>\n",
              "      <td>...</td>\n",
              "      <td>50.8</td>\n",
              "      <td>37.2</td>\n",
              "      <td>33.6</td>\n",
              "      <td>69.7</td>\n",
              "      <td>-7.3</td>\n",
              "      <td>NO POSTSEASON</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2023.0</td>\n",
              "      <td>8</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>545</td>\n",
              "      <td>Southeast Missouri St.</td>\n",
              "      <td>OVC</td>\n",
              "      <td>34</td>\n",
              "      <td>17</td>\n",
              "      <td>98.6</td>\n",
              "      <td>106.8</td>\n",
              "      <td>0.2856</td>\n",
              "      <td>50.1</td>\n",
              "      <td>51.3</td>\n",
              "      <td>...</td>\n",
              "      <td>50.7</td>\n",
              "      <td>33.2</td>\n",
              "      <td>35.0</td>\n",
              "      <td>72.8</td>\n",
              "      <td>-11.7</td>\n",
              "      <td>NO POSTSEASON</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2023.0</td>\n",
              "      <td>8</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>546</td>\n",
              "      <td>Fairleigh Dickinson</td>\n",
              "      <td>NEC</td>\n",
              "      <td>33</td>\n",
              "      <td>18</td>\n",
              "      <td>105.0</td>\n",
              "      <td>116.4</td>\n",
              "      <td>0.2330</td>\n",
              "      <td>51.7</td>\n",
              "      <td>55.4</td>\n",
              "      <td>...</td>\n",
              "      <td>56.1</td>\n",
              "      <td>34.7</td>\n",
              "      <td>36.2</td>\n",
              "      <td>69.4</td>\n",
              "      <td>-12.2</td>\n",
              "      <td>NO POSTSEASON</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2023.0</td>\n",
              "      <td>8</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>67 rows × 27 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-355de02f-6c23-4306-92c4-0fbc7e05e658')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-355de02f-6c23-4306-92c4-0fbc7e05e658 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-355de02f-6c23-4306-92c4-0fbc7e05e658');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalizing the Data"
      ],
      "metadata": {
        "id": "FJ1r9XO7nPv-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Normalizing the input values is an essential task to complete before training a network. Feeding inputs of different scales through the network is inefficient, as the weight adjustments caused by input values of one scale might not be optimal for values of another scale. This means that more training data will have to be fed through the model to reach the optimal weights for the model. However, if the input values are normalized before feeding them through the network, the weight changes made for one input will be applicable to all inputs, meaning that the network will reach its optimal weights quicker than if inputs of differing scales are fed through the model.\n",
        "\n",
        "\n",
        "To prevent issues with normalization, we remove certain columns from the data, such as those containing string data (e.g., team names). Additionally, we remove the columns \"G\" and \"W\" because 2023 Tournament Teams will not have played as many games as teams from 2013-2021 because they have yet to complete their season. Before removing them, we make copies of the data so that we can reattach them later. Furthermore, we exclude the \"postseasonNums\" column from the data as we don't want to normalize our response variable. We also create a copy of its values beforehand.\n"
      ],
      "metadata": {
        "id": "uqP1lx0wVbKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "postseasonResults = []\n",
        "years = []\n",
        "seeds = []\n",
        "teams = []\n",
        "for row, rowSeries in data.iterrows(): #making copies of certain feature values before dropping them from the dataframe\n",
        "  postseasonResults.append(rowSeries[\"PostseasonNums\"])\n",
        "  years.append(rowSeries[\"YEAR\"])\n",
        "  seeds.append(rowSeries[\"SEED\"])\n",
        "  teams.append(rowSeries[\"TEAM\"])\n",
        "refined = data.drop([\"G\", \"W\", \"TEAM\", \"CONF\", \"POSTSEASON\",\"YEAR\", \"PostseasonNums\", \"CONFB\"], axis =1) #dropping columns that do not need normalized from the data\n",
        "\n",
        "refinedMean = refined.mean()\n",
        "\n",
        "refinedStd = refined.std()\n",
        "\n",
        "\n",
        "normalizedRefined=(refined-refined.mean())/refined.std()  #normalizing the data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "currentYears = []\n",
        "currentSeeds = []\n",
        "currentTeams = []\n",
        "for row, rowSeries in currentTournamentTeams.iterrows(): #making copies of certain feature values before dropping them from the dataframe\n",
        "  currentYears.append(rowSeries[\"YEAR\"])\n",
        "  currentSeeds.append(rowSeries[\"SEED\"])\n",
        "  currentTeams.append(rowSeries[\"TEAM\"])\n",
        "\n",
        "refinedFinalTesting = currentTournamentTeams.drop([\"G\", \"W\",\"TEAM\", \"CONF\", \"POSTSEASON\",\"YEAR\", \"PostseasonNums\", \"CONFB\"], axis =1) #dropping columns that do not need normalized from the data\n",
        "\n",
        "normalizedFinalTesting = refinedFinalTesting.sub(refinedMean)\n",
        "normalizedFinalTesting= normalizedFinalTesting.div(refinedStd)\n",
        "\n",
        "\n",
        "\n",
        "normalizedRefined.head(10)\n"
      ],
      "metadata": {
        "id": "jJj7vlTEku-D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "4ec2e069-7153-4d45-a6e4-03af371a4870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      index     ADJOE     ADJDE   BARTHAG     EFG_O     EFG_D       TOR  \\\n",
              "0 -1.726645  2.146791 -1.115185  1.058822  1.658192 -0.487976 -0.660001   \n",
              "1 -1.719435  1.848148 -0.320494  0.921394  0.187444  0.167667 -1.148806   \n",
              "2 -1.712226  0.056290 -0.560750  0.523855 -0.768542 -0.160154  0.317610   \n",
              "3 -1.705017 -0.336661 -1.706583  0.779247 -1.614222 -1.512418 -0.171195   \n",
              "4 -1.697807  1.266580 -0.764043  0.907238 -0.180243 -0.938730  0.371922   \n",
              "5 -1.690598 -0.713894 -0.394419  0.053178  0.297750 -0.160154  0.208987   \n",
              "6 -1.683388  1.062246  1.028632  0.274951  1.216967  0.659399 -1.854859   \n",
              "7 -1.676179  1.706686  1.102556  0.508520  0.738974  2.052640 -0.605689   \n",
              "8 -1.668970  2.759795 -0.560750  1.055283  0.996355  0.003756 -2.778158   \n",
              "9 -1.661760  0.449242 -1.152147  0.829382  0.665437  0.003756 -1.909170   \n",
              "\n",
              "       TORD       ORB       DRB       FTR      FTRD      2P_O      2P_D  \\\n",
              "0 -0.182633  0.958021  0.404368  0.496310 -1.637261  1.624001 -0.101587   \n",
              "1 -0.349930  2.120697  0.335171 -0.901637 -0.529276  0.947422 -0.671703   \n",
              "2  0.570205  0.412275  2.168880 -0.323819 -0.938378 -1.319117  0.502065   \n",
              "3  0.946625  0.697012  0.611957  0.291278 -0.034944 -0.913170 -1.107673   \n",
              "4 -0.559052  1.954600  0.750350 -0.361097  0.732122 -0.067446 -0.336340   \n",
              "5  0.862976 -2.007989 -0.218402  2.043371 -0.495184  0.507646 -0.034514   \n",
              "6 -0.851823 -0.513120  0.473564 -1.609930  0.186653  0.676790  1.608761   \n",
              "7 -1.186418 -0.252111 -0.460590  0.664064  0.766214  0.338501  1.843514   \n",
              "8 -1.353715  0.080082 -1.844521 -0.174705 -1.892950  1.251882 -0.638166   \n",
              "9  0.193786 -1.485971 -1.429342 -1.199865 -0.597460  1.218053  0.066094   \n",
              "\n",
              "       3P_O      3P_D     ADJ_T       WAB      SEED  \n",
              "0  1.092349 -0.853074 -0.375475  1.855472 -1.668541  \n",
              "1 -1.268069  1.468820  1.273425  1.435138 -1.668541  \n",
              "2  0.030161 -1.191684 -0.655477 -0.346275  0.259273  \n",
              "3 -2.054874 -1.385175 -0.624365  0.874694 -1.025937  \n",
              "4 -0.048520 -1.530294  0.713421  1.014805 -1.025937  \n",
              "5 -0.127200 -0.320974 -1.184369 -0.126100  0.473475  \n",
              "6  1.092349 -1.530294 -0.126584  0.054043  0.259273  \n",
              "7  1.013668  1.275329  1.366759 -0.026021  0.473475  \n",
              "8  0.226862  2.097666 -2.584378  1.975567 -1.668541  \n",
              "9 -0.284561  0.017636 -0.531031  1.094869 -1.240138  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-43ae69e7-202a-4863-8645-8cf3303f4243\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>ADJOE</th>\n",
              "      <th>ADJDE</th>\n",
              "      <th>BARTHAG</th>\n",
              "      <th>EFG_O</th>\n",
              "      <th>EFG_D</th>\n",
              "      <th>TOR</th>\n",
              "      <th>TORD</th>\n",
              "      <th>ORB</th>\n",
              "      <th>DRB</th>\n",
              "      <th>FTR</th>\n",
              "      <th>FTRD</th>\n",
              "      <th>2P_O</th>\n",
              "      <th>2P_D</th>\n",
              "      <th>3P_O</th>\n",
              "      <th>3P_D</th>\n",
              "      <th>ADJ_T</th>\n",
              "      <th>WAB</th>\n",
              "      <th>SEED</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.726645</td>\n",
              "      <td>2.146791</td>\n",
              "      <td>-1.115185</td>\n",
              "      <td>1.058822</td>\n",
              "      <td>1.658192</td>\n",
              "      <td>-0.487976</td>\n",
              "      <td>-0.660001</td>\n",
              "      <td>-0.182633</td>\n",
              "      <td>0.958021</td>\n",
              "      <td>0.404368</td>\n",
              "      <td>0.496310</td>\n",
              "      <td>-1.637261</td>\n",
              "      <td>1.624001</td>\n",
              "      <td>-0.101587</td>\n",
              "      <td>1.092349</td>\n",
              "      <td>-0.853074</td>\n",
              "      <td>-0.375475</td>\n",
              "      <td>1.855472</td>\n",
              "      <td>-1.668541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.719435</td>\n",
              "      <td>1.848148</td>\n",
              "      <td>-0.320494</td>\n",
              "      <td>0.921394</td>\n",
              "      <td>0.187444</td>\n",
              "      <td>0.167667</td>\n",
              "      <td>-1.148806</td>\n",
              "      <td>-0.349930</td>\n",
              "      <td>2.120697</td>\n",
              "      <td>0.335171</td>\n",
              "      <td>-0.901637</td>\n",
              "      <td>-0.529276</td>\n",
              "      <td>0.947422</td>\n",
              "      <td>-0.671703</td>\n",
              "      <td>-1.268069</td>\n",
              "      <td>1.468820</td>\n",
              "      <td>1.273425</td>\n",
              "      <td>1.435138</td>\n",
              "      <td>-1.668541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.712226</td>\n",
              "      <td>0.056290</td>\n",
              "      <td>-0.560750</td>\n",
              "      <td>0.523855</td>\n",
              "      <td>-0.768542</td>\n",
              "      <td>-0.160154</td>\n",
              "      <td>0.317610</td>\n",
              "      <td>0.570205</td>\n",
              "      <td>0.412275</td>\n",
              "      <td>2.168880</td>\n",
              "      <td>-0.323819</td>\n",
              "      <td>-0.938378</td>\n",
              "      <td>-1.319117</td>\n",
              "      <td>0.502065</td>\n",
              "      <td>0.030161</td>\n",
              "      <td>-1.191684</td>\n",
              "      <td>-0.655477</td>\n",
              "      <td>-0.346275</td>\n",
              "      <td>0.259273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.705017</td>\n",
              "      <td>-0.336661</td>\n",
              "      <td>-1.706583</td>\n",
              "      <td>0.779247</td>\n",
              "      <td>-1.614222</td>\n",
              "      <td>-1.512418</td>\n",
              "      <td>-0.171195</td>\n",
              "      <td>0.946625</td>\n",
              "      <td>0.697012</td>\n",
              "      <td>0.611957</td>\n",
              "      <td>0.291278</td>\n",
              "      <td>-0.034944</td>\n",
              "      <td>-0.913170</td>\n",
              "      <td>-1.107673</td>\n",
              "      <td>-2.054874</td>\n",
              "      <td>-1.385175</td>\n",
              "      <td>-0.624365</td>\n",
              "      <td>0.874694</td>\n",
              "      <td>-1.025937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.697807</td>\n",
              "      <td>1.266580</td>\n",
              "      <td>-0.764043</td>\n",
              "      <td>0.907238</td>\n",
              "      <td>-0.180243</td>\n",
              "      <td>-0.938730</td>\n",
              "      <td>0.371922</td>\n",
              "      <td>-0.559052</td>\n",
              "      <td>1.954600</td>\n",
              "      <td>0.750350</td>\n",
              "      <td>-0.361097</td>\n",
              "      <td>0.732122</td>\n",
              "      <td>-0.067446</td>\n",
              "      <td>-0.336340</td>\n",
              "      <td>-0.048520</td>\n",
              "      <td>-1.530294</td>\n",
              "      <td>0.713421</td>\n",
              "      <td>1.014805</td>\n",
              "      <td>-1.025937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-1.690598</td>\n",
              "      <td>-0.713894</td>\n",
              "      <td>-0.394419</td>\n",
              "      <td>0.053178</td>\n",
              "      <td>0.297750</td>\n",
              "      <td>-0.160154</td>\n",
              "      <td>0.208987</td>\n",
              "      <td>0.862976</td>\n",
              "      <td>-2.007989</td>\n",
              "      <td>-0.218402</td>\n",
              "      <td>2.043371</td>\n",
              "      <td>-0.495184</td>\n",
              "      <td>0.507646</td>\n",
              "      <td>-0.034514</td>\n",
              "      <td>-0.127200</td>\n",
              "      <td>-0.320974</td>\n",
              "      <td>-1.184369</td>\n",
              "      <td>-0.126100</td>\n",
              "      <td>0.473475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-1.683388</td>\n",
              "      <td>1.062246</td>\n",
              "      <td>1.028632</td>\n",
              "      <td>0.274951</td>\n",
              "      <td>1.216967</td>\n",
              "      <td>0.659399</td>\n",
              "      <td>-1.854859</td>\n",
              "      <td>-0.851823</td>\n",
              "      <td>-0.513120</td>\n",
              "      <td>0.473564</td>\n",
              "      <td>-1.609930</td>\n",
              "      <td>0.186653</td>\n",
              "      <td>0.676790</td>\n",
              "      <td>1.608761</td>\n",
              "      <td>1.092349</td>\n",
              "      <td>-1.530294</td>\n",
              "      <td>-0.126584</td>\n",
              "      <td>0.054043</td>\n",
              "      <td>0.259273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-1.676179</td>\n",
              "      <td>1.706686</td>\n",
              "      <td>1.102556</td>\n",
              "      <td>0.508520</td>\n",
              "      <td>0.738974</td>\n",
              "      <td>2.052640</td>\n",
              "      <td>-0.605689</td>\n",
              "      <td>-1.186418</td>\n",
              "      <td>-0.252111</td>\n",
              "      <td>-0.460590</td>\n",
              "      <td>0.664064</td>\n",
              "      <td>0.766214</td>\n",
              "      <td>0.338501</td>\n",
              "      <td>1.843514</td>\n",
              "      <td>1.013668</td>\n",
              "      <td>1.275329</td>\n",
              "      <td>1.366759</td>\n",
              "      <td>-0.026021</td>\n",
              "      <td>0.473475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-1.668970</td>\n",
              "      <td>2.759795</td>\n",
              "      <td>-0.560750</td>\n",
              "      <td>1.055283</td>\n",
              "      <td>0.996355</td>\n",
              "      <td>0.003756</td>\n",
              "      <td>-2.778158</td>\n",
              "      <td>-1.353715</td>\n",
              "      <td>0.080082</td>\n",
              "      <td>-1.844521</td>\n",
              "      <td>-0.174705</td>\n",
              "      <td>-1.892950</td>\n",
              "      <td>1.251882</td>\n",
              "      <td>-0.638166</td>\n",
              "      <td>0.226862</td>\n",
              "      <td>2.097666</td>\n",
              "      <td>-2.584378</td>\n",
              "      <td>1.975567</td>\n",
              "      <td>-1.668541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>-1.661760</td>\n",
              "      <td>0.449242</td>\n",
              "      <td>-1.152147</td>\n",
              "      <td>0.829382</td>\n",
              "      <td>0.665437</td>\n",
              "      <td>0.003756</td>\n",
              "      <td>-1.909170</td>\n",
              "      <td>0.193786</td>\n",
              "      <td>-1.485971</td>\n",
              "      <td>-1.429342</td>\n",
              "      <td>-1.199865</td>\n",
              "      <td>-0.597460</td>\n",
              "      <td>1.218053</td>\n",
              "      <td>0.066094</td>\n",
              "      <td>-0.284561</td>\n",
              "      <td>0.017636</td>\n",
              "      <td>-0.531031</td>\n",
              "      <td>1.094869</td>\n",
              "      <td>-1.240138</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-43ae69e7-202a-4863-8645-8cf3303f4243')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-43ae69e7-202a-4863-8645-8cf3303f4243 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-43ae69e7-202a-4863-8645-8cf3303f4243');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normalizedFinalTesting.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "oFzMT8H0NTWm",
        "outputId": "a9660857-e28e-47ca-9aa2-40e4613cb0b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      index     ADJOE     ADJDE   BARTHAG     EFG_O     EFG_D       TOR  \\\n",
              "0  1.733854  0.873629 -1.595696  0.984505  0.224212 -2.127083 -1.203118   \n",
              "1  1.741063  0.606422 -1.540252  0.936729  0.224212 -2.536860  0.806416   \n",
              "2  1.748273  0.339215 -0.690118  0.674259  0.812512 -0.160154 -0.497065   \n",
              "3  1.755482  1.140836 -0.764043  0.883056  0.518362 -0.897753  0.752104   \n",
              "4  1.762692 -0.116608 -1.447846  0.770990 -0.915617 -0.569931 -0.062572   \n",
              "5  1.769901  0.999373 -0.542269  0.804020 -0.106706 -0.201132 -0.171195   \n",
              "6  1.777110  1.235144 -0.228089  0.781607  1.621423 -0.242110  0.480546   \n",
              "7  1.784320  0.889347  0.160016  0.566912 -0.217012  1.315042  0.534857   \n",
              "8  1.791529  0.103444 -0.338976  0.451307  1.069893 -0.733842 -0.442754   \n",
              "9  1.798738 -0.619586  0.917744 -0.810908  0.150675  0.126689 -0.334130   \n",
              "\n",
              "       TORD       ORB       DRB       FTR      FTRD      2P_O      2P_D  \\\n",
              "0  1.239395  1.266486 -0.564385 -1.609930  0.851444  0.710619 -1.174746   \n",
              "1 -1.311891  0.483460 -0.529786 -0.081508 -0.120174  1.184224 -1.811934   \n",
              "2 -1.981080 -1.485971 -1.982915 -1.684487 -2.336144  1.150395 -0.336340   \n",
              "3  0.151962  1.764776 -0.979564 -1.069390  0.885536  0.778277 -0.269268   \n",
              "4  0.068313 -0.014830 -1.083359 -0.752523 -0.699735 -0.710196  0.938036   \n",
              "5 -1.521012  1.574951 -1.913718  0.011688 -2.438419  1.015080  0.200239   \n",
              "6 -0.893647 -0.252111 -1.014162 -0.249262 -1.364526  1.793145 -0.135123   \n",
              "7  0.779327  0.530916  0.092983  0.421753  0.885536 -0.067446  1.407543   \n",
              "8 -0.308106 -0.323295 -1.567735 -1.423537 -1.159975  1.082738 -0.604630   \n",
              "9 -1.897432 -0.797857 -2.155906 -0.901637 -0.716781  0.812106  0.200239   \n",
              "\n",
              "       3P_O      3P_D     ADJ_T       WAB      SEED  \n",
              "0 -0.559943 -2.594495 -1.122146  1.315043 -1.668541  \n",
              "1 -0.835325 -2.449377  1.833429  1.755393 -1.668541  \n",
              "2  0.030161  0.452991  0.713421  1.715361 -0.597533  \n",
              "3 -0.087860 -1.530294  0.028972  0.694551 -1.025937  \n",
              "4 -0.795985 -2.594495 -0.593254  0.854678 -0.811735  \n",
              "5 -1.464770 -0.901447 -1.153258  1.595266 -1.668541  \n",
              "6  0.738286 -0.417719  1.428982  1.234980 -1.454340  \n",
              "7 -0.363242  0.404618  0.246752 -0.006005  0.045072  \n",
              "8  0.502244 -0.562838  0.402308  0.654519  0.045072  \n",
              "9 -0.756645 -0.030737  0.153418 -0.926735  1.330281  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c9b82199-55d6-4524-955c-ab33b32fe24a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>ADJOE</th>\n",
              "      <th>ADJDE</th>\n",
              "      <th>BARTHAG</th>\n",
              "      <th>EFG_O</th>\n",
              "      <th>EFG_D</th>\n",
              "      <th>TOR</th>\n",
              "      <th>TORD</th>\n",
              "      <th>ORB</th>\n",
              "      <th>DRB</th>\n",
              "      <th>FTR</th>\n",
              "      <th>FTRD</th>\n",
              "      <th>2P_O</th>\n",
              "      <th>2P_D</th>\n",
              "      <th>3P_O</th>\n",
              "      <th>3P_D</th>\n",
              "      <th>ADJ_T</th>\n",
              "      <th>WAB</th>\n",
              "      <th>SEED</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.733854</td>\n",
              "      <td>0.873629</td>\n",
              "      <td>-1.595696</td>\n",
              "      <td>0.984505</td>\n",
              "      <td>0.224212</td>\n",
              "      <td>-2.127083</td>\n",
              "      <td>-1.203118</td>\n",
              "      <td>1.239395</td>\n",
              "      <td>1.266486</td>\n",
              "      <td>-0.564385</td>\n",
              "      <td>-1.609930</td>\n",
              "      <td>0.851444</td>\n",
              "      <td>0.710619</td>\n",
              "      <td>-1.174746</td>\n",
              "      <td>-0.559943</td>\n",
              "      <td>-2.594495</td>\n",
              "      <td>-1.122146</td>\n",
              "      <td>1.315043</td>\n",
              "      <td>-1.668541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.741063</td>\n",
              "      <td>0.606422</td>\n",
              "      <td>-1.540252</td>\n",
              "      <td>0.936729</td>\n",
              "      <td>0.224212</td>\n",
              "      <td>-2.536860</td>\n",
              "      <td>0.806416</td>\n",
              "      <td>-1.311891</td>\n",
              "      <td>0.483460</td>\n",
              "      <td>-0.529786</td>\n",
              "      <td>-0.081508</td>\n",
              "      <td>-0.120174</td>\n",
              "      <td>1.184224</td>\n",
              "      <td>-1.811934</td>\n",
              "      <td>-0.835325</td>\n",
              "      <td>-2.449377</td>\n",
              "      <td>1.833429</td>\n",
              "      <td>1.755393</td>\n",
              "      <td>-1.668541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.748273</td>\n",
              "      <td>0.339215</td>\n",
              "      <td>-0.690118</td>\n",
              "      <td>0.674259</td>\n",
              "      <td>0.812512</td>\n",
              "      <td>-0.160154</td>\n",
              "      <td>-0.497065</td>\n",
              "      <td>-1.981080</td>\n",
              "      <td>-1.485971</td>\n",
              "      <td>-1.982915</td>\n",
              "      <td>-1.684487</td>\n",
              "      <td>-2.336144</td>\n",
              "      <td>1.150395</td>\n",
              "      <td>-0.336340</td>\n",
              "      <td>0.030161</td>\n",
              "      <td>0.452991</td>\n",
              "      <td>0.713421</td>\n",
              "      <td>1.715361</td>\n",
              "      <td>-0.597533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.755482</td>\n",
              "      <td>1.140836</td>\n",
              "      <td>-0.764043</td>\n",
              "      <td>0.883056</td>\n",
              "      <td>0.518362</td>\n",
              "      <td>-0.897753</td>\n",
              "      <td>0.752104</td>\n",
              "      <td>0.151962</td>\n",
              "      <td>1.764776</td>\n",
              "      <td>-0.979564</td>\n",
              "      <td>-1.069390</td>\n",
              "      <td>0.885536</td>\n",
              "      <td>0.778277</td>\n",
              "      <td>-0.269268</td>\n",
              "      <td>-0.087860</td>\n",
              "      <td>-1.530294</td>\n",
              "      <td>0.028972</td>\n",
              "      <td>0.694551</td>\n",
              "      <td>-1.025937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.762692</td>\n",
              "      <td>-0.116608</td>\n",
              "      <td>-1.447846</td>\n",
              "      <td>0.770990</td>\n",
              "      <td>-0.915617</td>\n",
              "      <td>-0.569931</td>\n",
              "      <td>-0.062572</td>\n",
              "      <td>0.068313</td>\n",
              "      <td>-0.014830</td>\n",
              "      <td>-1.083359</td>\n",
              "      <td>-0.752523</td>\n",
              "      <td>-0.699735</td>\n",
              "      <td>-0.710196</td>\n",
              "      <td>0.938036</td>\n",
              "      <td>-0.795985</td>\n",
              "      <td>-2.594495</td>\n",
              "      <td>-0.593254</td>\n",
              "      <td>0.854678</td>\n",
              "      <td>-0.811735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.769901</td>\n",
              "      <td>0.999373</td>\n",
              "      <td>-0.542269</td>\n",
              "      <td>0.804020</td>\n",
              "      <td>-0.106706</td>\n",
              "      <td>-0.201132</td>\n",
              "      <td>-0.171195</td>\n",
              "      <td>-1.521012</td>\n",
              "      <td>1.574951</td>\n",
              "      <td>-1.913718</td>\n",
              "      <td>0.011688</td>\n",
              "      <td>-2.438419</td>\n",
              "      <td>1.015080</td>\n",
              "      <td>0.200239</td>\n",
              "      <td>-1.464770</td>\n",
              "      <td>-0.901447</td>\n",
              "      <td>-1.153258</td>\n",
              "      <td>1.595266</td>\n",
              "      <td>-1.668541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1.777110</td>\n",
              "      <td>1.235144</td>\n",
              "      <td>-0.228089</td>\n",
              "      <td>0.781607</td>\n",
              "      <td>1.621423</td>\n",
              "      <td>-0.242110</td>\n",
              "      <td>0.480546</td>\n",
              "      <td>-0.893647</td>\n",
              "      <td>-0.252111</td>\n",
              "      <td>-1.014162</td>\n",
              "      <td>-0.249262</td>\n",
              "      <td>-1.364526</td>\n",
              "      <td>1.793145</td>\n",
              "      <td>-0.135123</td>\n",
              "      <td>0.738286</td>\n",
              "      <td>-0.417719</td>\n",
              "      <td>1.428982</td>\n",
              "      <td>1.234980</td>\n",
              "      <td>-1.454340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1.784320</td>\n",
              "      <td>0.889347</td>\n",
              "      <td>0.160016</td>\n",
              "      <td>0.566912</td>\n",
              "      <td>-0.217012</td>\n",
              "      <td>1.315042</td>\n",
              "      <td>0.534857</td>\n",
              "      <td>0.779327</td>\n",
              "      <td>0.530916</td>\n",
              "      <td>0.092983</td>\n",
              "      <td>0.421753</td>\n",
              "      <td>0.885536</td>\n",
              "      <td>-0.067446</td>\n",
              "      <td>1.407543</td>\n",
              "      <td>-0.363242</td>\n",
              "      <td>0.404618</td>\n",
              "      <td>0.246752</td>\n",
              "      <td>-0.006005</td>\n",
              "      <td>0.045072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.791529</td>\n",
              "      <td>0.103444</td>\n",
              "      <td>-0.338976</td>\n",
              "      <td>0.451307</td>\n",
              "      <td>1.069893</td>\n",
              "      <td>-0.733842</td>\n",
              "      <td>-0.442754</td>\n",
              "      <td>-0.308106</td>\n",
              "      <td>-0.323295</td>\n",
              "      <td>-1.567735</td>\n",
              "      <td>-1.423537</td>\n",
              "      <td>-1.159975</td>\n",
              "      <td>1.082738</td>\n",
              "      <td>-0.604630</td>\n",
              "      <td>0.502244</td>\n",
              "      <td>-0.562838</td>\n",
              "      <td>0.402308</td>\n",
              "      <td>0.654519</td>\n",
              "      <td>0.045072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.798738</td>\n",
              "      <td>-0.619586</td>\n",
              "      <td>0.917744</td>\n",
              "      <td>-0.810908</td>\n",
              "      <td>0.150675</td>\n",
              "      <td>0.126689</td>\n",
              "      <td>-0.334130</td>\n",
              "      <td>-1.897432</td>\n",
              "      <td>-0.797857</td>\n",
              "      <td>-2.155906</td>\n",
              "      <td>-0.901637</td>\n",
              "      <td>-0.716781</td>\n",
              "      <td>0.812106</td>\n",
              "      <td>0.200239</td>\n",
              "      <td>-0.756645</td>\n",
              "      <td>-0.030737</td>\n",
              "      <td>0.153418</td>\n",
              "      <td>-0.926735</td>\n",
              "      <td>1.330281</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c9b82199-55d6-4524-955c-ab33b32fe24a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c9b82199-55d6-4524-955c-ab33b32fe24a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c9b82199-55d6-4524-955c-ab33b32fe24a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below reappends the postseason results of the teams in our training data, so that it can be used as an output variables for our network."
      ],
      "metadata": {
        "id": "JoxGv2NsKUtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalizedRefined[\"PostseasonResults\"] = 0\n",
        "for i in range(0, len(postseasonResults)):\n",
        "  normalizedRefined.at[i,\"PostseasonResults\"] = postseasonResults[i]\n",
        "normalizedRefined = normalizedRefined.drop([\"index\"], axis=1)"
      ],
      "metadata": {
        "id": "2oAmagkwXEgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalizedRefined"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "8VAAv2aVncy6",
        "outputId": "bfc977ac-2948-4d72-8a8b-4160dda1f62d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        ADJOE     ADJDE   BARTHAG     EFG_O     EFG_D       TOR      TORD  \\\n",
              "0    2.146791 -1.115185  1.058822  1.658192 -0.487976 -0.660001 -0.182633   \n",
              "1    1.848148 -0.320494  0.921394  0.187444  0.167667 -1.148806 -0.349930   \n",
              "2    0.056290 -0.560750  0.523855 -0.768542 -0.160154  0.317610  0.570205   \n",
              "3   -0.336661 -1.706583  0.779247 -1.614222 -1.512418 -0.171195  0.946625   \n",
              "4    1.266580 -0.764043  0.907238 -0.180243 -0.938730  0.371922 -0.559052   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "475 -0.022300 -1.632658  0.839409 -0.547930 -1.307529  1.784027  0.068313   \n",
              "476  0.449242 -0.819486  0.743268  0.150675 -0.446998  1.077975  0.319259   \n",
              "477 -1.059691 -1.484809  0.447768 -1.025923 -0.528954  2.109897  1.239395   \n",
              "478  0.072008 -0.080239  0.322726 -0.069937  0.659399 -0.225507  0.946625   \n",
              "479 -1.279744 -0.061758 -0.599753 -0.180243 -0.324065  1.892650  1.281219   \n",
              "\n",
              "          ORB       DRB       FTR      FTRD      2P_O      2P_D      3P_O  \\\n",
              "0    0.958021  0.404368  0.496310 -1.637261  1.624001 -0.101587  1.092349   \n",
              "1    2.120697  0.335171 -0.901637 -0.529276  0.947422 -0.671703 -1.268069   \n",
              "2    0.412275  2.168880 -0.323819 -0.938378 -1.319117  0.502065  0.030161   \n",
              "3    0.697012  0.611957  0.291278 -0.034944 -0.913170 -1.107673 -2.054874   \n",
              "4    1.954600  0.750350 -0.361097  0.732122 -0.067446 -0.336340 -0.048520   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "475  1.029205 -0.495188 -0.100147 -0.188357 -0.236591 -0.772311 -0.717305   \n",
              "476  0.768197 -0.806573  0.048967 -0.103128 -0.168933 -1.074137  0.462904   \n",
              "477  0.958021 -0.633581  0.235360 -0.034944 -0.676367 -0.571094 -1.032027   \n",
              "478 -0.655488  1.788299 -1.088030 -0.853148 -0.608709  1.340471  0.698946   \n",
              "479  0.174995  1.303923 -0.361097 -0.137220  0.406159  0.099631 -0.992687   \n",
              "\n",
              "         3P_D     ADJ_T       WAB      SEED  PostseasonResults  \n",
              "0   -0.853074 -0.375475  1.855472 -1.668541                  0  \n",
              "1    1.468820  1.273425  1.435138 -1.668541                  1  \n",
              "2   -1.191684 -0.655477 -0.346275  0.259273                  2  \n",
              "3   -1.385175 -0.624365  0.874694 -1.025937                  3  \n",
              "4   -1.530294  0.713421  1.014805 -1.025937                  4  \n",
              "..        ...       ...       ...       ...                ...  \n",
              "475 -1.481921 -0.997701  1.054837 -1.240138                  4  \n",
              "476  1.275329 -0.251029  0.634503 -0.597533                  4  \n",
              "477  0.114382  0.495642  0.294233  0.687676                  4  \n",
              "478 -1.433548 -0.499920 -0.226179  0.901878                  4  \n",
              "479 -0.901447  0.464531 -1.086862  1.330281                  4  \n",
              "\n",
              "[480 rows x 19 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e0769371-3cbd-4f52-a534-c88662eca0b4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ADJOE</th>\n",
              "      <th>ADJDE</th>\n",
              "      <th>BARTHAG</th>\n",
              "      <th>EFG_O</th>\n",
              "      <th>EFG_D</th>\n",
              "      <th>TOR</th>\n",
              "      <th>TORD</th>\n",
              "      <th>ORB</th>\n",
              "      <th>DRB</th>\n",
              "      <th>FTR</th>\n",
              "      <th>FTRD</th>\n",
              "      <th>2P_O</th>\n",
              "      <th>2P_D</th>\n",
              "      <th>3P_O</th>\n",
              "      <th>3P_D</th>\n",
              "      <th>ADJ_T</th>\n",
              "      <th>WAB</th>\n",
              "      <th>SEED</th>\n",
              "      <th>PostseasonResults</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.146791</td>\n",
              "      <td>-1.115185</td>\n",
              "      <td>1.058822</td>\n",
              "      <td>1.658192</td>\n",
              "      <td>-0.487976</td>\n",
              "      <td>-0.660001</td>\n",
              "      <td>-0.182633</td>\n",
              "      <td>0.958021</td>\n",
              "      <td>0.404368</td>\n",
              "      <td>0.496310</td>\n",
              "      <td>-1.637261</td>\n",
              "      <td>1.624001</td>\n",
              "      <td>-0.101587</td>\n",
              "      <td>1.092349</td>\n",
              "      <td>-0.853074</td>\n",
              "      <td>-0.375475</td>\n",
              "      <td>1.855472</td>\n",
              "      <td>-1.668541</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.848148</td>\n",
              "      <td>-0.320494</td>\n",
              "      <td>0.921394</td>\n",
              "      <td>0.187444</td>\n",
              "      <td>0.167667</td>\n",
              "      <td>-1.148806</td>\n",
              "      <td>-0.349930</td>\n",
              "      <td>2.120697</td>\n",
              "      <td>0.335171</td>\n",
              "      <td>-0.901637</td>\n",
              "      <td>-0.529276</td>\n",
              "      <td>0.947422</td>\n",
              "      <td>-0.671703</td>\n",
              "      <td>-1.268069</td>\n",
              "      <td>1.468820</td>\n",
              "      <td>1.273425</td>\n",
              "      <td>1.435138</td>\n",
              "      <td>-1.668541</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.056290</td>\n",
              "      <td>-0.560750</td>\n",
              "      <td>0.523855</td>\n",
              "      <td>-0.768542</td>\n",
              "      <td>-0.160154</td>\n",
              "      <td>0.317610</td>\n",
              "      <td>0.570205</td>\n",
              "      <td>0.412275</td>\n",
              "      <td>2.168880</td>\n",
              "      <td>-0.323819</td>\n",
              "      <td>-0.938378</td>\n",
              "      <td>-1.319117</td>\n",
              "      <td>0.502065</td>\n",
              "      <td>0.030161</td>\n",
              "      <td>-1.191684</td>\n",
              "      <td>-0.655477</td>\n",
              "      <td>-0.346275</td>\n",
              "      <td>0.259273</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.336661</td>\n",
              "      <td>-1.706583</td>\n",
              "      <td>0.779247</td>\n",
              "      <td>-1.614222</td>\n",
              "      <td>-1.512418</td>\n",
              "      <td>-0.171195</td>\n",
              "      <td>0.946625</td>\n",
              "      <td>0.697012</td>\n",
              "      <td>0.611957</td>\n",
              "      <td>0.291278</td>\n",
              "      <td>-0.034944</td>\n",
              "      <td>-0.913170</td>\n",
              "      <td>-1.107673</td>\n",
              "      <td>-2.054874</td>\n",
              "      <td>-1.385175</td>\n",
              "      <td>-0.624365</td>\n",
              "      <td>0.874694</td>\n",
              "      <td>-1.025937</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.266580</td>\n",
              "      <td>-0.764043</td>\n",
              "      <td>0.907238</td>\n",
              "      <td>-0.180243</td>\n",
              "      <td>-0.938730</td>\n",
              "      <td>0.371922</td>\n",
              "      <td>-0.559052</td>\n",
              "      <td>1.954600</td>\n",
              "      <td>0.750350</td>\n",
              "      <td>-0.361097</td>\n",
              "      <td>0.732122</td>\n",
              "      <td>-0.067446</td>\n",
              "      <td>-0.336340</td>\n",
              "      <td>-0.048520</td>\n",
              "      <td>-1.530294</td>\n",
              "      <td>0.713421</td>\n",
              "      <td>1.014805</td>\n",
              "      <td>-1.025937</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>475</th>\n",
              "      <td>-0.022300</td>\n",
              "      <td>-1.632658</td>\n",
              "      <td>0.839409</td>\n",
              "      <td>-0.547930</td>\n",
              "      <td>-1.307529</td>\n",
              "      <td>1.784027</td>\n",
              "      <td>0.068313</td>\n",
              "      <td>1.029205</td>\n",
              "      <td>-0.495188</td>\n",
              "      <td>-0.100147</td>\n",
              "      <td>-0.188357</td>\n",
              "      <td>-0.236591</td>\n",
              "      <td>-0.772311</td>\n",
              "      <td>-0.717305</td>\n",
              "      <td>-1.481921</td>\n",
              "      <td>-0.997701</td>\n",
              "      <td>1.054837</td>\n",
              "      <td>-1.240138</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>476</th>\n",
              "      <td>0.449242</td>\n",
              "      <td>-0.819486</td>\n",
              "      <td>0.743268</td>\n",
              "      <td>0.150675</td>\n",
              "      <td>-0.446998</td>\n",
              "      <td>1.077975</td>\n",
              "      <td>0.319259</td>\n",
              "      <td>0.768197</td>\n",
              "      <td>-0.806573</td>\n",
              "      <td>0.048967</td>\n",
              "      <td>-0.103128</td>\n",
              "      <td>-0.168933</td>\n",
              "      <td>-1.074137</td>\n",
              "      <td>0.462904</td>\n",
              "      <td>1.275329</td>\n",
              "      <td>-0.251029</td>\n",
              "      <td>0.634503</td>\n",
              "      <td>-0.597533</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>477</th>\n",
              "      <td>-1.059691</td>\n",
              "      <td>-1.484809</td>\n",
              "      <td>0.447768</td>\n",
              "      <td>-1.025923</td>\n",
              "      <td>-0.528954</td>\n",
              "      <td>2.109897</td>\n",
              "      <td>1.239395</td>\n",
              "      <td>0.958021</td>\n",
              "      <td>-0.633581</td>\n",
              "      <td>0.235360</td>\n",
              "      <td>-0.034944</td>\n",
              "      <td>-0.676367</td>\n",
              "      <td>-0.571094</td>\n",
              "      <td>-1.032027</td>\n",
              "      <td>0.114382</td>\n",
              "      <td>0.495642</td>\n",
              "      <td>0.294233</td>\n",
              "      <td>0.687676</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>478</th>\n",
              "      <td>0.072008</td>\n",
              "      <td>-0.080239</td>\n",
              "      <td>0.322726</td>\n",
              "      <td>-0.069937</td>\n",
              "      <td>0.659399</td>\n",
              "      <td>-0.225507</td>\n",
              "      <td>0.946625</td>\n",
              "      <td>-0.655488</td>\n",
              "      <td>1.788299</td>\n",
              "      <td>-1.088030</td>\n",
              "      <td>-0.853148</td>\n",
              "      <td>-0.608709</td>\n",
              "      <td>1.340471</td>\n",
              "      <td>0.698946</td>\n",
              "      <td>-1.433548</td>\n",
              "      <td>-0.499920</td>\n",
              "      <td>-0.226179</td>\n",
              "      <td>0.901878</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>479</th>\n",
              "      <td>-1.279744</td>\n",
              "      <td>-0.061758</td>\n",
              "      <td>-0.599753</td>\n",
              "      <td>-0.180243</td>\n",
              "      <td>-0.324065</td>\n",
              "      <td>1.892650</td>\n",
              "      <td>1.281219</td>\n",
              "      <td>0.174995</td>\n",
              "      <td>1.303923</td>\n",
              "      <td>-0.361097</td>\n",
              "      <td>-0.137220</td>\n",
              "      <td>0.406159</td>\n",
              "      <td>0.099631</td>\n",
              "      <td>-0.992687</td>\n",
              "      <td>-0.901447</td>\n",
              "      <td>0.464531</td>\n",
              "      <td>-1.086862</td>\n",
              "      <td>1.330281</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>480 rows × 19 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e0769371-3cbd-4f52-a534-c88662eca0b4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e0769371-3cbd-4f52-a534-c88662eca0b4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e0769371-3cbd-4f52-a534-c88662eca0b4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting the Data"
      ],
      "metadata": {
        "id": "E8lGo07GoToq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now split the data into a training, validation and testing set. 60% of Tournament Team Data from 2013-2021 will be fed into the training set, 20% will be fed into the validation set and 20% will be fed into the testing set.\n",
        "\n",
        "We utilize a validation set to help prevent our network from overfitting the training data. If the network were to overlearn the training data, it would not be able to generalize and accurately predict data it had never been faced with before."
      ],
      "metadata": {
        "id": "BG1ioye5bikA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def splitData(data, testingPercentage, validationPercentage):\n",
        "    '''\n",
        "    This function splits inputted data into three separate datasets, each of which are returned.\n",
        "    It takes 3 inputs:\n",
        "      data : the data one wishes to be split\n",
        "      testingPercentage : the percentage of the original data one wants to be allocated for testing\n",
        "      validationPercentage : the percentage of the original data one wants to be allocated for validation\n",
        "    The function will return three separate datasets, one for the training data, one for the testing and one for the validation\n",
        "    '''\n",
        "\n",
        "    shuffled_indices = np.random.permutation(len(data)) #randomly arranges the observations within the data\n",
        "    test_set_size = int(len(data)*testingPercentage) #calculates the number of observations needed to create a proper testing set\n",
        "    validation_set_size = int(len(data)*validationPercentage)\n",
        "    training_set_size = len(data)-test_set_size - validation_set_size\n",
        "    train_indices = shuffled_indices[:training_set_size] #allocates the testing and training indices\n",
        "    validation_indices = shuffled_indices[training_set_size:training_set_size+validation_set_size]\n",
        "    test_indices = shuffled_indices[training_set_size+validation_set_size:]\n",
        "    return data.iloc[train_indices], data.iloc[test_indices],data.iloc[validation_indices]  #training, testing, validation datasets are returned (in that order)\n"
      ],
      "metadata": {
        "id": "aJoi8dl3nres"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training, testing, validation = splitData(normalizedRefined, 0.20, 0.20) #creates 3 new datasets, one for training, one for testing and one for validation\n"
      ],
      "metadata": {
        "id": "scdUUqY-oXLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainingX = training.drop([\"PostseasonResults\"], axis = 1)\n",
        "trainingY = training[[\"PostseasonResults\"]]\n",
        "validationX = validation.drop([\"PostseasonResults\"], axis = 1)\n",
        "validationY = validation[[\"PostseasonResults\"]]\n",
        "testingX = testing.drop([\"PostseasonResults\"], axis = 1)\n",
        "testingY = testing[[\"PostseasonResults\"]]"
      ],
      "metadata": {
        "id": "9AwC89Bhq65O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainingX.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "kmFZElWytR55",
        "outputId": "d9eff131-b748-4dd9-c80c-d52712d61f73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        ADJOE     ADJDE   BARTHAG     EFG_O     EFG_D       TOR      TORD  \\\n",
              "293  0.857911  0.695970  0.329804  1.547886  0.905265 -0.008260 -0.977296   \n",
              "283 -0.446687  1.878766 -1.457941  1.621423  1.478953  1.512468 -0.224457   \n",
              "34   1.077964 -0.080239  0.704340  0.077138  0.495488 -0.768624  0.486557   \n",
              "380 -1.075409 -0.653156 -0.019369 -0.915617 -2.127083 -1.311741 -2.064729   \n",
              "430  0.024854 -1.059742  0.694313 -0.584699 -0.201132  0.752104  0.528381   \n",
              "\n",
              "          ORB       DRB       FTR      FTRD      2P_O      2P_D      3P_O  \\\n",
              "293  0.341091 -1.360145 -0.044229 -0.392909  1.894632  0.367920  0.384223   \n",
              "283 -0.679216  0.196778 -0.920276 -0.904286  1.793145  1.541688  0.698946   \n",
              "34   0.554644  0.508162  0.589506 -0.256541  0.507646 -0.168659 -0.599284   \n",
              "380 -0.584304 -1.636932 -0.267901 -0.699735 -0.946999 -1.845470 -0.048520   \n",
              "430  0.246179 -0.702778 -0.249262  0.374158 -0.067446 -0.436949 -1.071367   \n",
              "\n",
              "         3P_D     ADJ_T       WAB      SEED  \n",
              "293  1.275329  0.215640  0.474376 -1.025937  \n",
              "283  0.549737 -0.002139 -1.226973  1.330281  \n",
              "34   1.468820  0.433420  1.054837 -1.668541  \n",
              "380 -1.288430 -1.308814 -0.086068  0.473475  \n",
              "430  0.162754  0.744533  1.234980 -1.025937  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f53500fc-7be7-4070-bd77-980b8163b750\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ADJOE</th>\n",
              "      <th>ADJDE</th>\n",
              "      <th>BARTHAG</th>\n",
              "      <th>EFG_O</th>\n",
              "      <th>EFG_D</th>\n",
              "      <th>TOR</th>\n",
              "      <th>TORD</th>\n",
              "      <th>ORB</th>\n",
              "      <th>DRB</th>\n",
              "      <th>FTR</th>\n",
              "      <th>FTRD</th>\n",
              "      <th>2P_O</th>\n",
              "      <th>2P_D</th>\n",
              "      <th>3P_O</th>\n",
              "      <th>3P_D</th>\n",
              "      <th>ADJ_T</th>\n",
              "      <th>WAB</th>\n",
              "      <th>SEED</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>0.857911</td>\n",
              "      <td>0.695970</td>\n",
              "      <td>0.329804</td>\n",
              "      <td>1.547886</td>\n",
              "      <td>0.905265</td>\n",
              "      <td>-0.008260</td>\n",
              "      <td>-0.977296</td>\n",
              "      <td>0.341091</td>\n",
              "      <td>-1.360145</td>\n",
              "      <td>-0.044229</td>\n",
              "      <td>-0.392909</td>\n",
              "      <td>1.894632</td>\n",
              "      <td>0.367920</td>\n",
              "      <td>0.384223</td>\n",
              "      <td>1.275329</td>\n",
              "      <td>0.215640</td>\n",
              "      <td>0.474376</td>\n",
              "      <td>-1.025937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>283</th>\n",
              "      <td>-0.446687</td>\n",
              "      <td>1.878766</td>\n",
              "      <td>-1.457941</td>\n",
              "      <td>1.621423</td>\n",
              "      <td>1.478953</td>\n",
              "      <td>1.512468</td>\n",
              "      <td>-0.224457</td>\n",
              "      <td>-0.679216</td>\n",
              "      <td>0.196778</td>\n",
              "      <td>-0.920276</td>\n",
              "      <td>-0.904286</td>\n",
              "      <td>1.793145</td>\n",
              "      <td>1.541688</td>\n",
              "      <td>0.698946</td>\n",
              "      <td>0.549737</td>\n",
              "      <td>-0.002139</td>\n",
              "      <td>-1.226973</td>\n",
              "      <td>1.330281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>1.077964</td>\n",
              "      <td>-0.080239</td>\n",
              "      <td>0.704340</td>\n",
              "      <td>0.077138</td>\n",
              "      <td>0.495488</td>\n",
              "      <td>-0.768624</td>\n",
              "      <td>0.486557</td>\n",
              "      <td>0.554644</td>\n",
              "      <td>0.508162</td>\n",
              "      <td>0.589506</td>\n",
              "      <td>-0.256541</td>\n",
              "      <td>0.507646</td>\n",
              "      <td>-0.168659</td>\n",
              "      <td>-0.599284</td>\n",
              "      <td>1.468820</td>\n",
              "      <td>0.433420</td>\n",
              "      <td>1.054837</td>\n",
              "      <td>-1.668541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>380</th>\n",
              "      <td>-1.075409</td>\n",
              "      <td>-0.653156</td>\n",
              "      <td>-0.019369</td>\n",
              "      <td>-0.915617</td>\n",
              "      <td>-2.127083</td>\n",
              "      <td>-1.311741</td>\n",
              "      <td>-2.064729</td>\n",
              "      <td>-0.584304</td>\n",
              "      <td>-1.636932</td>\n",
              "      <td>-0.267901</td>\n",
              "      <td>-0.699735</td>\n",
              "      <td>-0.946999</td>\n",
              "      <td>-1.845470</td>\n",
              "      <td>-0.048520</td>\n",
              "      <td>-1.288430</td>\n",
              "      <td>-1.308814</td>\n",
              "      <td>-0.086068</td>\n",
              "      <td>0.473475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>430</th>\n",
              "      <td>0.024854</td>\n",
              "      <td>-1.059742</td>\n",
              "      <td>0.694313</td>\n",
              "      <td>-0.584699</td>\n",
              "      <td>-0.201132</td>\n",
              "      <td>0.752104</td>\n",
              "      <td>0.528381</td>\n",
              "      <td>0.246179</td>\n",
              "      <td>-0.702778</td>\n",
              "      <td>-0.249262</td>\n",
              "      <td>0.374158</td>\n",
              "      <td>-0.067446</td>\n",
              "      <td>-0.436949</td>\n",
              "      <td>-1.071367</td>\n",
              "      <td>0.162754</td>\n",
              "      <td>0.744533</td>\n",
              "      <td>1.234980</td>\n",
              "      <td>-1.025937</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f53500fc-7be7-4070-bd77-980b8163b750')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f53500fc-7be7-4070-bd77-980b8163b750 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f53500fc-7be7-4070-bd77-980b8163b750');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construction of the MLP"
      ],
      "metadata": {
        "id": "AgMvzm5hOBeB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We construct the MLP using an input layer, two hidden layers, and an output layer.\n",
        "\n",
        "\n",
        "\n",
        "*   Input Layer\n",
        "  *   Made up of 18 nodes for each of the training data's features\n",
        "\n",
        "*   First Hidden Layer\n",
        "  *   Made up of 26 nodes that use the relu function to determine if they activate or not\n",
        "*   Second Hidden Layer\n",
        "  *   Made up of 18 nodes that also use the relu function to determine if they activate\n",
        "*   Output layer\n",
        "  *   Made up of 8 output nodes (one for each possible tournament outcome). The possible outcomes are listed below. Uses the soft-max activation function to compute the chance an observation belongs to each of the 8 possible outcomes listed below.\n",
        "    *  Round of 68, Round of 64, Round of 32, Sweet 16, Elite 8, Final 4, Runner Up, Champion\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gPdjtOpSJ9RU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(18, input_dim=18, activation='relu'))\n",
        "model.add(keras.layers.Dense(36, activation = \"relu\"))\n",
        "model.add(keras.layers.Dense(18, activation = \"relu\"))\n",
        "model.add(keras.layers.Dense(8, activation = \"softmax\"))\n",
        "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"sgd\", metrics = [\"accuracy\"])"
      ],
      "metadata": {
        "id": "83I1SVysrL5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to determine when we need to stop training the model, we first train it an excessive number of times. In doing so we can see the point at which the validation loss begins to increase. Using the graph below as an aid, we can see that the validation loss begins to increase at around epoch 120."
      ],
      "metadata": {
        "id": "0xzm0YRgOHEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(trainingX, trainingY, epochs = 400, validation_data = (validationX, validationY))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ul5UOEaVrzMu",
        "outputId": "225c6108-df49-4822-a11d-4528b1b6851c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "9/9 [==============================] - 2s 56ms/step - loss: 1.9610 - accuracy: 0.1944 - val_loss: 1.9542 - val_accuracy: 0.2708\n",
            "Epoch 2/400\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1.8937 - accuracy: 0.3229 - val_loss: 1.8849 - val_accuracy: 0.3958\n",
            "Epoch 3/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1.8350 - accuracy: 0.3681 - val_loss: 1.8234 - val_accuracy: 0.4792\n",
            "Epoch 4/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.7860 - accuracy: 0.4028 - val_loss: 1.7713 - val_accuracy: 0.5104\n",
            "Epoch 5/400\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 1.7447 - accuracy: 0.4271 - val_loss: 1.7268 - val_accuracy: 0.5417\n",
            "Epoch 6/400\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 1.7105 - accuracy: 0.4306 - val_loss: 1.6888 - val_accuracy: 0.5417\n",
            "Epoch 7/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.6816 - accuracy: 0.4410 - val_loss: 1.6575 - val_accuracy: 0.5417\n",
            "Epoch 8/400\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 1.6576 - accuracy: 0.4410 - val_loss: 1.6314 - val_accuracy: 0.5417\n",
            "Epoch 9/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.6375 - accuracy: 0.4410 - val_loss: 1.6089 - val_accuracy: 0.5521\n",
            "Epoch 10/400\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 1.6201 - accuracy: 0.4410 - val_loss: 1.5891 - val_accuracy: 0.5521\n",
            "Epoch 11/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1.6051 - accuracy: 0.4410 - val_loss: 1.5721 - val_accuracy: 0.5521\n",
            "Epoch 12/400\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 1.5915 - accuracy: 0.4410 - val_loss: 1.5577 - val_accuracy: 0.5521\n",
            "Epoch 13/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.5778 - accuracy: 0.4410 - val_loss: 1.5448 - val_accuracy: 0.5625\n",
            "Epoch 14/400\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1.5659 - accuracy: 0.4410 - val_loss: 1.5335 - val_accuracy: 0.5625\n",
            "Epoch 15/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.5551 - accuracy: 0.4410 - val_loss: 1.5230 - val_accuracy: 0.5729\n",
            "Epoch 16/400\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1.5452 - accuracy: 0.4410 - val_loss: 1.5135 - val_accuracy: 0.5729\n",
            "Epoch 17/400\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 1.5359 - accuracy: 0.4444 - val_loss: 1.5055 - val_accuracy: 0.5729\n",
            "Epoch 18/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1.5263 - accuracy: 0.4444 - val_loss: 1.4974 - val_accuracy: 0.5729\n",
            "Epoch 19/400\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1.5182 - accuracy: 0.4444 - val_loss: 1.4899 - val_accuracy: 0.5729\n",
            "Epoch 20/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.5097 - accuracy: 0.4444 - val_loss: 1.4836 - val_accuracy: 0.5729\n",
            "Epoch 21/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.5023 - accuracy: 0.4444 - val_loss: 1.4780 - val_accuracy: 0.5729\n",
            "Epoch 22/400\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1.4947 - accuracy: 0.4410 - val_loss: 1.4721 - val_accuracy: 0.5729\n",
            "Epoch 23/400\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 1.4877 - accuracy: 0.4479 - val_loss: 1.4669 - val_accuracy: 0.5729\n",
            "Epoch 24/400\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1.4805 - accuracy: 0.4479 - val_loss: 1.4632 - val_accuracy: 0.5729\n",
            "Epoch 25/400\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 1.4742 - accuracy: 0.4410 - val_loss: 1.4585 - val_accuracy: 0.5625\n",
            "Epoch 26/400\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 1.4677 - accuracy: 0.4444 - val_loss: 1.4539 - val_accuracy: 0.5625\n",
            "Epoch 27/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.4615 - accuracy: 0.4444 - val_loss: 1.4501 - val_accuracy: 0.5625\n",
            "Epoch 28/400\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 1.4553 - accuracy: 0.4444 - val_loss: 1.4471 - val_accuracy: 0.5625\n",
            "Epoch 29/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.4504 - accuracy: 0.4514 - val_loss: 1.4435 - val_accuracy: 0.5625\n",
            "Epoch 30/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.4449 - accuracy: 0.4583 - val_loss: 1.4395 - val_accuracy: 0.5625\n",
            "Epoch 31/400\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 1.4399 - accuracy: 0.4618 - val_loss: 1.4358 - val_accuracy: 0.5625\n",
            "Epoch 32/400\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1.4350 - accuracy: 0.4653 - val_loss: 1.4320 - val_accuracy: 0.5625\n",
            "Epoch 33/400\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 1.4290 - accuracy: 0.4653 - val_loss: 1.4292 - val_accuracy: 0.5729\n",
            "Epoch 34/400\n",
            "9/9 [==============================] - 0s 35ms/step - loss: 1.4245 - accuracy: 0.4653 - val_loss: 1.4266 - val_accuracy: 0.5625\n",
            "Epoch 35/400\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 1.4192 - accuracy: 0.4688 - val_loss: 1.4237 - val_accuracy: 0.5625\n",
            "Epoch 36/400\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 1.4150 - accuracy: 0.4722 - val_loss: 1.4207 - val_accuracy: 0.5521\n",
            "Epoch 37/400\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 1.4106 - accuracy: 0.4722 - val_loss: 1.4177 - val_accuracy: 0.5521\n",
            "Epoch 38/400\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1.4058 - accuracy: 0.4722 - val_loss: 1.4160 - val_accuracy: 0.5521\n",
            "Epoch 39/400\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1.4019 - accuracy: 0.4757 - val_loss: 1.4136 - val_accuracy: 0.5521\n",
            "Epoch 40/400\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 1.3971 - accuracy: 0.4722 - val_loss: 1.4113 - val_accuracy: 0.5521\n",
            "Epoch 41/400\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 1.3928 - accuracy: 0.4757 - val_loss: 1.4085 - val_accuracy: 0.5312\n",
            "Epoch 42/400\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 1.3881 - accuracy: 0.4757 - val_loss: 1.4057 - val_accuracy: 0.5312\n",
            "Epoch 43/400\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 1.3837 - accuracy: 0.4792 - val_loss: 1.4039 - val_accuracy: 0.5312\n",
            "Epoch 44/400\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 1.3799 - accuracy: 0.4826 - val_loss: 1.4016 - val_accuracy: 0.5208\n",
            "Epoch 45/400\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 1.3761 - accuracy: 0.4826 - val_loss: 1.4001 - val_accuracy: 0.5208\n",
            "Epoch 46/400\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 1.3712 - accuracy: 0.4792 - val_loss: 1.3986 - val_accuracy: 0.5104\n",
            "Epoch 47/400\n",
            "9/9 [==============================] - 0s 38ms/step - loss: 1.3671 - accuracy: 0.4792 - val_loss: 1.3966 - val_accuracy: 0.5104\n",
            "Epoch 48/400\n",
            "9/9 [==============================] - 1s 94ms/step - loss: 1.3633 - accuracy: 0.4826 - val_loss: 1.3941 - val_accuracy: 0.5104\n",
            "Epoch 49/400\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1.3599 - accuracy: 0.4826 - val_loss: 1.3937 - val_accuracy: 0.5104\n",
            "Epoch 50/400\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 1.3553 - accuracy: 0.4792 - val_loss: 1.3919 - val_accuracy: 0.5104\n",
            "Epoch 51/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.3507 - accuracy: 0.4826 - val_loss: 1.3897 - val_accuracy: 0.5104\n",
            "Epoch 52/400\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1.3475 - accuracy: 0.4826 - val_loss: 1.3884 - val_accuracy: 0.5208\n",
            "Epoch 53/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.3433 - accuracy: 0.4792 - val_loss: 1.3862 - val_accuracy: 0.5208\n",
            "Epoch 54/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1.3396 - accuracy: 0.4826 - val_loss: 1.3855 - val_accuracy: 0.5312\n",
            "Epoch 55/400\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1.3357 - accuracy: 0.4826 - val_loss: 1.3841 - val_accuracy: 0.5312\n",
            "Epoch 56/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.3321 - accuracy: 0.4792 - val_loss: 1.3825 - val_accuracy: 0.5208\n",
            "Epoch 57/400\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 1.3287 - accuracy: 0.4826 - val_loss: 1.3816 - val_accuracy: 0.5208\n",
            "Epoch 58/400\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 1.3245 - accuracy: 0.4792 - val_loss: 1.3788 - val_accuracy: 0.5208\n",
            "Epoch 59/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.3213 - accuracy: 0.4826 - val_loss: 1.3773 - val_accuracy: 0.5208\n",
            "Epoch 60/400\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 1.3180 - accuracy: 0.4826 - val_loss: 1.3762 - val_accuracy: 0.5208\n",
            "Epoch 61/400\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 1.3141 - accuracy: 0.4792 - val_loss: 1.3730 - val_accuracy: 0.5208\n",
            "Epoch 62/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.3098 - accuracy: 0.4896 - val_loss: 1.3719 - val_accuracy: 0.5208\n",
            "Epoch 63/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.3080 - accuracy: 0.4861 - val_loss: 1.3712 - val_accuracy: 0.5104\n",
            "Epoch 64/400\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 1.3031 - accuracy: 0.4896 - val_loss: 1.3693 - val_accuracy: 0.5104\n",
            "Epoch 65/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1.2997 - accuracy: 0.4896 - val_loss: 1.3691 - val_accuracy: 0.5000\n",
            "Epoch 66/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.2964 - accuracy: 0.4826 - val_loss: 1.3675 - val_accuracy: 0.5000\n",
            "Epoch 67/400\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 1.2930 - accuracy: 0.4861 - val_loss: 1.3660 - val_accuracy: 0.5000\n",
            "Epoch 68/400\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 1.2895 - accuracy: 0.4826 - val_loss: 1.3651 - val_accuracy: 0.5000\n",
            "Epoch 69/400\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1.2874 - accuracy: 0.4861 - val_loss: 1.3637 - val_accuracy: 0.5000\n",
            "Epoch 70/400\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 1.2838 - accuracy: 0.4826 - val_loss: 1.3610 - val_accuracy: 0.5000\n",
            "Epoch 71/400\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 1.2797 - accuracy: 0.4861 - val_loss: 1.3598 - val_accuracy: 0.5000\n",
            "Epoch 72/400\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1.2767 - accuracy: 0.4861 - val_loss: 1.3602 - val_accuracy: 0.5000\n",
            "Epoch 73/400\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 1.2740 - accuracy: 0.4861 - val_loss: 1.3609 - val_accuracy: 0.5000\n",
            "Epoch 74/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1.2699 - accuracy: 0.4896 - val_loss: 1.3602 - val_accuracy: 0.5000\n",
            "Epoch 75/400\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1.2677 - accuracy: 0.4931 - val_loss: 1.3583 - val_accuracy: 0.5000\n",
            "Epoch 76/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.2650 - accuracy: 0.4931 - val_loss: 1.3575 - val_accuracy: 0.5000\n",
            "Epoch 77/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.2618 - accuracy: 0.4965 - val_loss: 1.3548 - val_accuracy: 0.5000\n",
            "Epoch 78/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1.2595 - accuracy: 0.4896 - val_loss: 1.3545 - val_accuracy: 0.5000\n",
            "Epoch 79/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.2572 - accuracy: 0.4965 - val_loss: 1.3522 - val_accuracy: 0.5000\n",
            "Epoch 80/400\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 1.2538 - accuracy: 0.4931 - val_loss: 1.3523 - val_accuracy: 0.5000\n",
            "Epoch 81/400\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 1.2503 - accuracy: 0.4965 - val_loss: 1.3533 - val_accuracy: 0.5000\n",
            "Epoch 82/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.2483 - accuracy: 0.4896 - val_loss: 1.3535 - val_accuracy: 0.5000\n",
            "Epoch 83/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.2462 - accuracy: 0.4965 - val_loss: 1.3526 - val_accuracy: 0.5000\n",
            "Epoch 84/400\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1.2438 - accuracy: 0.4965 - val_loss: 1.3503 - val_accuracy: 0.5000\n",
            "Epoch 85/400\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 1.2403 - accuracy: 0.4965 - val_loss: 1.3508 - val_accuracy: 0.5000\n",
            "Epoch 86/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1.2380 - accuracy: 0.4965 - val_loss: 1.3482 - val_accuracy: 0.5000\n",
            "Epoch 87/400\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 1.2359 - accuracy: 0.4965 - val_loss: 1.3487 - val_accuracy: 0.5000\n",
            "Epoch 88/400\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1.2335 - accuracy: 0.4965 - val_loss: 1.3489 - val_accuracy: 0.5104\n",
            "Epoch 89/400\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1.2299 - accuracy: 0.4965 - val_loss: 1.3477 - val_accuracy: 0.5104\n",
            "Epoch 90/400\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1.2277 - accuracy: 0.4965 - val_loss: 1.3480 - val_accuracy: 0.5000\n",
            "Epoch 91/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.2248 - accuracy: 0.4965 - val_loss: 1.3477 - val_accuracy: 0.5000\n",
            "Epoch 92/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.2229 - accuracy: 0.4965 - val_loss: 1.3471 - val_accuracy: 0.5000\n",
            "Epoch 93/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.2209 - accuracy: 0.4965 - val_loss: 1.3446 - val_accuracy: 0.5104\n",
            "Epoch 94/400\n",
            "9/9 [==============================] - 0s 32ms/step - loss: 1.2181 - accuracy: 0.4965 - val_loss: 1.3472 - val_accuracy: 0.5000\n",
            "Epoch 95/400\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 1.2163 - accuracy: 0.4965 - val_loss: 1.3483 - val_accuracy: 0.5104\n",
            "Epoch 96/400\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 1.2138 - accuracy: 0.4965 - val_loss: 1.3493 - val_accuracy: 0.5104\n",
            "Epoch 97/400\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1.2104 - accuracy: 0.5035 - val_loss: 1.3453 - val_accuracy: 0.5104\n",
            "Epoch 98/400\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 1.2087 - accuracy: 0.5000 - val_loss: 1.3449 - val_accuracy: 0.5104\n",
            "Epoch 99/400\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 1.2068 - accuracy: 0.4965 - val_loss: 1.3462 - val_accuracy: 0.5104\n",
            "Epoch 100/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.2045 - accuracy: 0.5035 - val_loss: 1.3452 - val_accuracy: 0.5104\n",
            "Epoch 101/400\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 1.2042 - accuracy: 0.4965 - val_loss: 1.3440 - val_accuracy: 0.5104\n",
            "Epoch 102/400\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 1.2005 - accuracy: 0.5035 - val_loss: 1.3413 - val_accuracy: 0.5104\n",
            "Epoch 103/400\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 1.1991 - accuracy: 0.5000 - val_loss: 1.3412 - val_accuracy: 0.5104\n",
            "Epoch 104/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1.1970 - accuracy: 0.5035 - val_loss: 1.3410 - val_accuracy: 0.5104\n",
            "Epoch 105/400\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 1.1939 - accuracy: 0.5000 - val_loss: 1.3422 - val_accuracy: 0.5104\n",
            "Epoch 106/400\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 1.1915 - accuracy: 0.5069 - val_loss: 1.3399 - val_accuracy: 0.5104\n",
            "Epoch 107/400\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 1.1905 - accuracy: 0.5069 - val_loss: 1.3387 - val_accuracy: 0.5104\n",
            "Epoch 108/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1.1883 - accuracy: 0.4931 - val_loss: 1.3414 - val_accuracy: 0.5104\n",
            "Epoch 109/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.1867 - accuracy: 0.5000 - val_loss: 1.3401 - val_accuracy: 0.5104\n",
            "Epoch 110/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.1868 - accuracy: 0.5104 - val_loss: 1.3383 - val_accuracy: 0.5104\n",
            "Epoch 111/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.1823 - accuracy: 0.5069 - val_loss: 1.3385 - val_accuracy: 0.5104\n",
            "Epoch 112/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.1803 - accuracy: 0.5104 - val_loss: 1.3393 - val_accuracy: 0.5312\n",
            "Epoch 113/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.1788 - accuracy: 0.5104 - val_loss: 1.3382 - val_accuracy: 0.5104\n",
            "Epoch 114/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.1765 - accuracy: 0.5104 - val_loss: 1.3392 - val_accuracy: 0.5312\n",
            "Epoch 115/400\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.1753 - accuracy: 0.5139 - val_loss: 1.3387 - val_accuracy: 0.5312\n",
            "Epoch 116/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.1730 - accuracy: 0.5104 - val_loss: 1.3363 - val_accuracy: 0.5208\n",
            "Epoch 117/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.1715 - accuracy: 0.5035 - val_loss: 1.3351 - val_accuracy: 0.5104\n",
            "Epoch 118/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.1696 - accuracy: 0.5208 - val_loss: 1.3360 - val_accuracy: 0.5312\n",
            "Epoch 119/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.1681 - accuracy: 0.5069 - val_loss: 1.3341 - val_accuracy: 0.5208\n",
            "Epoch 120/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.1660 - accuracy: 0.5208 - val_loss: 1.3354 - val_accuracy: 0.5312\n",
            "Epoch 121/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.1662 - accuracy: 0.5243 - val_loss: 1.3363 - val_accuracy: 0.5312\n",
            "Epoch 122/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.1639 - accuracy: 0.5174 - val_loss: 1.3363 - val_accuracy: 0.5312\n",
            "Epoch 123/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.1613 - accuracy: 0.5139 - val_loss: 1.3390 - val_accuracy: 0.5312\n",
            "Epoch 124/400\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.1617 - accuracy: 0.5139 - val_loss: 1.3426 - val_accuracy: 0.5312\n",
            "Epoch 125/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.1587 - accuracy: 0.5208 - val_loss: 1.3391 - val_accuracy: 0.5312\n",
            "Epoch 126/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.1572 - accuracy: 0.5174 - val_loss: 1.3363 - val_accuracy: 0.5312\n",
            "Epoch 127/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.1550 - accuracy: 0.5278 - val_loss: 1.3403 - val_accuracy: 0.5312\n",
            "Epoch 128/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.1532 - accuracy: 0.5174 - val_loss: 1.3399 - val_accuracy: 0.5312\n",
            "Epoch 129/400\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.1514 - accuracy: 0.5278 - val_loss: 1.3347 - val_accuracy: 0.5312\n",
            "Epoch 130/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.1504 - accuracy: 0.5208 - val_loss: 1.3333 - val_accuracy: 0.5312\n",
            "Epoch 131/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.1489 - accuracy: 0.5208 - val_loss: 1.3348 - val_accuracy: 0.5312\n",
            "Epoch 132/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.1460 - accuracy: 0.5278 - val_loss: 1.3327 - val_accuracy: 0.5312\n",
            "Epoch 133/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.1447 - accuracy: 0.5208 - val_loss: 1.3329 - val_accuracy: 0.5312\n",
            "Epoch 134/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.1451 - accuracy: 0.5312 - val_loss: 1.3380 - val_accuracy: 0.5312\n",
            "Epoch 135/400\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.1421 - accuracy: 0.5347 - val_loss: 1.3365 - val_accuracy: 0.5312\n",
            "Epoch 136/400\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.1408 - accuracy: 0.5278 - val_loss: 1.3385 - val_accuracy: 0.5312\n",
            "Epoch 137/400\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.1390 - accuracy: 0.5382 - val_loss: 1.3371 - val_accuracy: 0.5312\n",
            "Epoch 138/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.1379 - accuracy: 0.5312 - val_loss: 1.3344 - val_accuracy: 0.5312\n",
            "Epoch 139/400\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.1360 - accuracy: 0.5312 - val_loss: 1.3328 - val_accuracy: 0.5312\n",
            "Epoch 140/400\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.1351 - accuracy: 0.5347 - val_loss: 1.3320 - val_accuracy: 0.5312\n",
            "Epoch 141/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.1329 - accuracy: 0.5347 - val_loss: 1.3346 - val_accuracy: 0.5312\n",
            "Epoch 142/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.1313 - accuracy: 0.5312 - val_loss: 1.3320 - val_accuracy: 0.5312\n",
            "Epoch 143/400\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.1306 - accuracy: 0.5278 - val_loss: 1.3376 - val_accuracy: 0.5312\n",
            "Epoch 144/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.1293 - accuracy: 0.5347 - val_loss: 1.3354 - val_accuracy: 0.5312\n",
            "Epoch 145/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.1262 - accuracy: 0.5382 - val_loss: 1.3353 - val_accuracy: 0.5312\n",
            "Epoch 146/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1.1267 - accuracy: 0.5347 - val_loss: 1.3343 - val_accuracy: 0.5312\n",
            "Epoch 147/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.1248 - accuracy: 0.5347 - val_loss: 1.3346 - val_accuracy: 0.5312\n",
            "Epoch 148/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.1211 - accuracy: 0.5382 - val_loss: 1.3365 - val_accuracy: 0.5312\n",
            "Epoch 149/400\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 1.1214 - accuracy: 0.5347 - val_loss: 1.3370 - val_accuracy: 0.5417\n",
            "Epoch 150/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1.1183 - accuracy: 0.5382 - val_loss: 1.3366 - val_accuracy: 0.5417\n",
            "Epoch 151/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.1172 - accuracy: 0.5486 - val_loss: 1.3321 - val_accuracy: 0.5417\n",
            "Epoch 152/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1.1167 - accuracy: 0.5382 - val_loss: 1.3353 - val_accuracy: 0.5417\n",
            "Epoch 153/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.1150 - accuracy: 0.5451 - val_loss: 1.3358 - val_accuracy: 0.5417\n",
            "Epoch 154/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1.1142 - accuracy: 0.5451 - val_loss: 1.3350 - val_accuracy: 0.5417\n",
            "Epoch 155/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.1118 - accuracy: 0.5382 - val_loss: 1.3353 - val_accuracy: 0.5417\n",
            "Epoch 156/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.1103 - accuracy: 0.5382 - val_loss: 1.3342 - val_accuracy: 0.5417\n",
            "Epoch 157/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1.1087 - accuracy: 0.5486 - val_loss: 1.3331 - val_accuracy: 0.5417\n",
            "Epoch 158/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.1087 - accuracy: 0.5382 - val_loss: 1.3348 - val_accuracy: 0.5417\n",
            "Epoch 159/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1.1064 - accuracy: 0.5382 - val_loss: 1.3364 - val_accuracy: 0.5417\n",
            "Epoch 160/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.1046 - accuracy: 0.5486 - val_loss: 1.3382 - val_accuracy: 0.5312\n",
            "Epoch 161/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1.1026 - accuracy: 0.5486 - val_loss: 1.3323 - val_accuracy: 0.5417\n",
            "Epoch 162/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1.1027 - accuracy: 0.5417 - val_loss: 1.3348 - val_accuracy: 0.5417\n",
            "Epoch 163/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.1020 - accuracy: 0.5486 - val_loss: 1.3341 - val_accuracy: 0.5417\n",
            "Epoch 164/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.1001 - accuracy: 0.5556 - val_loss: 1.3376 - val_accuracy: 0.5312\n",
            "Epoch 165/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.0979 - accuracy: 0.5590 - val_loss: 1.3340 - val_accuracy: 0.5417\n",
            "Epoch 166/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.0975 - accuracy: 0.5486 - val_loss: 1.3386 - val_accuracy: 0.5312\n",
            "Epoch 167/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.0957 - accuracy: 0.5451 - val_loss: 1.3382 - val_accuracy: 0.5312\n",
            "Epoch 168/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.0939 - accuracy: 0.5590 - val_loss: 1.3353 - val_accuracy: 0.5417\n",
            "Epoch 169/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.0909 - accuracy: 0.5625 - val_loss: 1.3368 - val_accuracy: 0.5312\n",
            "Epoch 170/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.0905 - accuracy: 0.5451 - val_loss: 1.3364 - val_accuracy: 0.5312\n",
            "Epoch 171/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1.0890 - accuracy: 0.5590 - val_loss: 1.3385 - val_accuracy: 0.5312\n",
            "Epoch 172/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.0876 - accuracy: 0.5556 - val_loss: 1.3377 - val_accuracy: 0.5312\n",
            "Epoch 173/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.0864 - accuracy: 0.5521 - val_loss: 1.3389 - val_accuracy: 0.5312\n",
            "Epoch 174/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1.0858 - accuracy: 0.5521 - val_loss: 1.3407 - val_accuracy: 0.5312\n",
            "Epoch 175/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.0829 - accuracy: 0.5556 - val_loss: 1.3411 - val_accuracy: 0.5312\n",
            "Epoch 176/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.0825 - accuracy: 0.5556 - val_loss: 1.3372 - val_accuracy: 0.5208\n",
            "Epoch 177/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.0808 - accuracy: 0.5556 - val_loss: 1.3398 - val_accuracy: 0.5208\n",
            "Epoch 178/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.0798 - accuracy: 0.5625 - val_loss: 1.3399 - val_accuracy: 0.5208\n",
            "Epoch 179/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.0767 - accuracy: 0.5694 - val_loss: 1.3387 - val_accuracy: 0.5208\n",
            "Epoch 180/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.0776 - accuracy: 0.5556 - val_loss: 1.3370 - val_accuracy: 0.5208\n",
            "Epoch 181/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1.0744 - accuracy: 0.5660 - val_loss: 1.3378 - val_accuracy: 0.5208\n",
            "Epoch 182/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.0741 - accuracy: 0.5660 - val_loss: 1.3364 - val_accuracy: 0.5208\n",
            "Epoch 183/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1.0733 - accuracy: 0.5590 - val_loss: 1.3392 - val_accuracy: 0.5208\n",
            "Epoch 184/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.0703 - accuracy: 0.5590 - val_loss: 1.3393 - val_accuracy: 0.5208\n",
            "Epoch 185/400\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.0696 - accuracy: 0.5590 - val_loss: 1.3395 - val_accuracy: 0.5208\n",
            "Epoch 186/400\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.0673 - accuracy: 0.5729 - val_loss: 1.3412 - val_accuracy: 0.5208\n",
            "Epoch 187/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.0654 - accuracy: 0.5660 - val_loss: 1.3414 - val_accuracy: 0.5208\n",
            "Epoch 188/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.0636 - accuracy: 0.5729 - val_loss: 1.3430 - val_accuracy: 0.5208\n",
            "Epoch 189/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.0642 - accuracy: 0.5625 - val_loss: 1.3391 - val_accuracy: 0.5104\n",
            "Epoch 190/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.0617 - accuracy: 0.5729 - val_loss: 1.3375 - val_accuracy: 0.5104\n",
            "Epoch 191/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.0605 - accuracy: 0.5694 - val_loss: 1.3429 - val_accuracy: 0.5104\n",
            "Epoch 192/400\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.0588 - accuracy: 0.5833 - val_loss: 1.3389 - val_accuracy: 0.5000\n",
            "Epoch 193/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.0569 - accuracy: 0.5799 - val_loss: 1.3436 - val_accuracy: 0.5000\n",
            "Epoch 194/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.0564 - accuracy: 0.5868 - val_loss: 1.3427 - val_accuracy: 0.5000\n",
            "Epoch 195/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.0549 - accuracy: 0.5729 - val_loss: 1.3391 - val_accuracy: 0.5000\n",
            "Epoch 196/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.0529 - accuracy: 0.6007 - val_loss: 1.3424 - val_accuracy: 0.5000\n",
            "Epoch 197/400\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.0518 - accuracy: 0.6007 - val_loss: 1.3410 - val_accuracy: 0.5000\n",
            "Epoch 198/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.0513 - accuracy: 0.5868 - val_loss: 1.3424 - val_accuracy: 0.5000\n",
            "Epoch 199/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.0489 - accuracy: 0.5903 - val_loss: 1.3417 - val_accuracy: 0.5000\n",
            "Epoch 200/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.0485 - accuracy: 0.5972 - val_loss: 1.3403 - val_accuracy: 0.5000\n",
            "Epoch 201/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.0454 - accuracy: 0.6111 - val_loss: 1.3399 - val_accuracy: 0.5000\n",
            "Epoch 202/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.0470 - accuracy: 0.5938 - val_loss: 1.3450 - val_accuracy: 0.5000\n",
            "Epoch 203/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.0423 - accuracy: 0.6007 - val_loss: 1.3457 - val_accuracy: 0.5104\n",
            "Epoch 204/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.0400 - accuracy: 0.6007 - val_loss: 1.3440 - val_accuracy: 0.5000\n",
            "Epoch 205/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.0402 - accuracy: 0.5972 - val_loss: 1.3491 - val_accuracy: 0.5208\n",
            "Epoch 206/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1.0404 - accuracy: 0.6042 - val_loss: 1.3464 - val_accuracy: 0.5104\n",
            "Epoch 207/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1.0381 - accuracy: 0.5938 - val_loss: 1.3485 - val_accuracy: 0.5208\n",
            "Epoch 208/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.0357 - accuracy: 0.6076 - val_loss: 1.3502 - val_accuracy: 0.5208\n",
            "Epoch 209/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.0339 - accuracy: 0.6042 - val_loss: 1.3482 - val_accuracy: 0.5208\n",
            "Epoch 210/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.0319 - accuracy: 0.5972 - val_loss: 1.3447 - val_accuracy: 0.5104\n",
            "Epoch 211/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.0307 - accuracy: 0.6007 - val_loss: 1.3451 - val_accuracy: 0.5104\n",
            "Epoch 212/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.0299 - accuracy: 0.5972 - val_loss: 1.3472 - val_accuracy: 0.5208\n",
            "Epoch 213/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.0333 - accuracy: 0.6076 - val_loss: 1.3479 - val_accuracy: 0.5104\n",
            "Epoch 214/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.0275 - accuracy: 0.5938 - val_loss: 1.3483 - val_accuracy: 0.5208\n",
            "Epoch 215/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.0256 - accuracy: 0.6042 - val_loss: 1.3537 - val_accuracy: 0.5104\n",
            "Epoch 216/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.0257 - accuracy: 0.5972 - val_loss: 1.3541 - val_accuracy: 0.5104\n",
            "Epoch 217/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.0226 - accuracy: 0.6111 - val_loss: 1.3496 - val_accuracy: 0.5104\n",
            "Epoch 218/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.0219 - accuracy: 0.6042 - val_loss: 1.3503 - val_accuracy: 0.5208\n",
            "Epoch 219/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.0197 - accuracy: 0.5972 - val_loss: 1.3504 - val_accuracy: 0.5208\n",
            "Epoch 220/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.0167 - accuracy: 0.6111 - val_loss: 1.3541 - val_accuracy: 0.5104\n",
            "Epoch 221/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.0181 - accuracy: 0.6076 - val_loss: 1.3516 - val_accuracy: 0.5208\n",
            "Epoch 222/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.0153 - accuracy: 0.5972 - val_loss: 1.3519 - val_accuracy: 0.5208\n",
            "Epoch 223/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.0122 - accuracy: 0.6111 - val_loss: 1.3507 - val_accuracy: 0.5104\n",
            "Epoch 224/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.0117 - accuracy: 0.6042 - val_loss: 1.3531 - val_accuracy: 0.5104\n",
            "Epoch 225/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.0092 - accuracy: 0.6111 - val_loss: 1.3538 - val_accuracy: 0.5208\n",
            "Epoch 226/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.0078 - accuracy: 0.6042 - val_loss: 1.3552 - val_accuracy: 0.5208\n",
            "Epoch 227/400\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.0063 - accuracy: 0.6146 - val_loss: 1.3544 - val_accuracy: 0.5208\n",
            "Epoch 228/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.0066 - accuracy: 0.6111 - val_loss: 1.3575 - val_accuracy: 0.5208\n",
            "Epoch 229/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.0065 - accuracy: 0.6042 - val_loss: 1.3607 - val_accuracy: 0.5208\n",
            "Epoch 230/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.0030 - accuracy: 0.6250 - val_loss: 1.3570 - val_accuracy: 0.5208\n",
            "Epoch 231/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.0034 - accuracy: 0.6007 - val_loss: 1.3606 - val_accuracy: 0.5208\n",
            "Epoch 232/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.0003 - accuracy: 0.6111 - val_loss: 1.3607 - val_accuracy: 0.5208\n",
            "Epoch 233/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1.0009 - accuracy: 0.6181 - val_loss: 1.3620 - val_accuracy: 0.5208\n",
            "Epoch 234/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.9979 - accuracy: 0.6076 - val_loss: 1.3601 - val_accuracy: 0.5312\n",
            "Epoch 235/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.9960 - accuracy: 0.6146 - val_loss: 1.3653 - val_accuracy: 0.5312\n",
            "Epoch 236/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.9953 - accuracy: 0.6181 - val_loss: 1.3627 - val_accuracy: 0.5312\n",
            "Epoch 237/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.9922 - accuracy: 0.6215 - val_loss: 1.3630 - val_accuracy: 0.5312\n",
            "Epoch 238/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.9898 - accuracy: 0.6076 - val_loss: 1.3646 - val_accuracy: 0.5312\n",
            "Epoch 239/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.9903 - accuracy: 0.6215 - val_loss: 1.3693 - val_accuracy: 0.5312\n",
            "Epoch 240/400\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.9884 - accuracy: 0.6146 - val_loss: 1.3704 - val_accuracy: 0.5312\n",
            "Epoch 241/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.9860 - accuracy: 0.6354 - val_loss: 1.3648 - val_accuracy: 0.5312\n",
            "Epoch 242/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.9877 - accuracy: 0.6076 - val_loss: 1.3671 - val_accuracy: 0.5312\n",
            "Epoch 243/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.9832 - accuracy: 0.6250 - val_loss: 1.3673 - val_accuracy: 0.5312\n",
            "Epoch 244/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.9819 - accuracy: 0.6181 - val_loss: 1.3712 - val_accuracy: 0.5312\n",
            "Epoch 245/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.9841 - accuracy: 0.6146 - val_loss: 1.3673 - val_accuracy: 0.5312\n",
            "Epoch 246/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.9830 - accuracy: 0.6076 - val_loss: 1.3730 - val_accuracy: 0.5208\n",
            "Epoch 247/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.9771 - accuracy: 0.6493 - val_loss: 1.3677 - val_accuracy: 0.5417\n",
            "Epoch 248/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.9772 - accuracy: 0.6389 - val_loss: 1.3689 - val_accuracy: 0.5417\n",
            "Epoch 249/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.9758 - accuracy: 0.6389 - val_loss: 1.3693 - val_accuracy: 0.5417\n",
            "Epoch 250/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.9741 - accuracy: 0.6181 - val_loss: 1.3736 - val_accuracy: 0.5208\n",
            "Epoch 251/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.9736 - accuracy: 0.6215 - val_loss: 1.3784 - val_accuracy: 0.5208\n",
            "Epoch 252/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.9709 - accuracy: 0.6389 - val_loss: 1.3762 - val_accuracy: 0.5208\n",
            "Epoch 253/400\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.9695 - accuracy: 0.6528 - val_loss: 1.3728 - val_accuracy: 0.5417\n",
            "Epoch 254/400\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.9682 - accuracy: 0.6354 - val_loss: 1.3732 - val_accuracy: 0.5417\n",
            "Epoch 255/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.9667 - accuracy: 0.6250 - val_loss: 1.3773 - val_accuracy: 0.5208\n",
            "Epoch 256/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.9661 - accuracy: 0.6319 - val_loss: 1.3810 - val_accuracy: 0.5208\n",
            "Epoch 257/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.9630 - accuracy: 0.6493 - val_loss: 1.3792 - val_accuracy: 0.5208\n",
            "Epoch 258/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.9640 - accuracy: 0.6458 - val_loss: 1.3843 - val_accuracy: 0.5208\n",
            "Epoch 259/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.9619 - accuracy: 0.6562 - val_loss: 1.3799 - val_accuracy: 0.5312\n",
            "Epoch 260/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.9593 - accuracy: 0.6389 - val_loss: 1.3871 - val_accuracy: 0.5104\n",
            "Epoch 261/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.9589 - accuracy: 0.6562 - val_loss: 1.3852 - val_accuracy: 0.5208\n",
            "Epoch 262/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.9553 - accuracy: 0.6632 - val_loss: 1.3818 - val_accuracy: 0.5312\n",
            "Epoch 263/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.9560 - accuracy: 0.6354 - val_loss: 1.3878 - val_accuracy: 0.5208\n",
            "Epoch 264/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.9539 - accuracy: 0.6597 - val_loss: 1.3842 - val_accuracy: 0.5312\n",
            "Epoch 265/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.9538 - accuracy: 0.6562 - val_loss: 1.3897 - val_accuracy: 0.5208\n",
            "Epoch 266/400\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.9525 - accuracy: 0.6562 - val_loss: 1.3902 - val_accuracy: 0.5208\n",
            "Epoch 267/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.9494 - accuracy: 0.6528 - val_loss: 1.3919 - val_accuracy: 0.5208\n",
            "Epoch 268/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.9508 - accuracy: 0.6493 - val_loss: 1.3949 - val_accuracy: 0.5104\n",
            "Epoch 269/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.9465 - accuracy: 0.6632 - val_loss: 1.3927 - val_accuracy: 0.5208\n",
            "Epoch 270/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.9456 - accuracy: 0.6458 - val_loss: 1.3985 - val_accuracy: 0.5104\n",
            "Epoch 271/400\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.9441 - accuracy: 0.6562 - val_loss: 1.3969 - val_accuracy: 0.5104\n",
            "Epoch 272/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.9420 - accuracy: 0.6632 - val_loss: 1.4018 - val_accuracy: 0.5208\n",
            "Epoch 273/400\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.9431 - accuracy: 0.6736 - val_loss: 1.3958 - val_accuracy: 0.5312\n",
            "Epoch 274/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.9389 - accuracy: 0.6736 - val_loss: 1.3991 - val_accuracy: 0.5208\n",
            "Epoch 275/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.9374 - accuracy: 0.6667 - val_loss: 1.3966 - val_accuracy: 0.5417\n",
            "Epoch 276/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.9379 - accuracy: 0.6493 - val_loss: 1.3999 - val_accuracy: 0.5208\n",
            "Epoch 277/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.9361 - accuracy: 0.6667 - val_loss: 1.3997 - val_accuracy: 0.5208\n",
            "Epoch 278/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.9340 - accuracy: 0.6562 - val_loss: 1.4024 - val_accuracy: 0.5208\n",
            "Epoch 279/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.9358 - accuracy: 0.6493 - val_loss: 1.4059 - val_accuracy: 0.5208\n",
            "Epoch 280/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.9318 - accuracy: 0.6667 - val_loss: 1.4045 - val_accuracy: 0.5208\n",
            "Epoch 281/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.9298 - accuracy: 0.6667 - val_loss: 1.4061 - val_accuracy: 0.5208\n",
            "Epoch 282/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.9299 - accuracy: 0.6597 - val_loss: 1.4118 - val_accuracy: 0.5208\n",
            "Epoch 283/400\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.9286 - accuracy: 0.6701 - val_loss: 1.4076 - val_accuracy: 0.5208\n",
            "Epoch 284/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.9259 - accuracy: 0.6528 - val_loss: 1.4078 - val_accuracy: 0.5312\n",
            "Epoch 285/400\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.9230 - accuracy: 0.6632 - val_loss: 1.4093 - val_accuracy: 0.5208\n",
            "Epoch 286/400\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.9222 - accuracy: 0.6597 - val_loss: 1.4122 - val_accuracy: 0.5208\n",
            "Epoch 287/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.9204 - accuracy: 0.6632 - val_loss: 1.4111 - val_accuracy: 0.5312\n",
            "Epoch 288/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.9197 - accuracy: 0.6632 - val_loss: 1.4099 - val_accuracy: 0.5417\n",
            "Epoch 289/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.9198 - accuracy: 0.6562 - val_loss: 1.4139 - val_accuracy: 0.5312\n",
            "Epoch 290/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.9200 - accuracy: 0.6701 - val_loss: 1.4118 - val_accuracy: 0.5417\n",
            "Epoch 291/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.9194 - accuracy: 0.6458 - val_loss: 1.4137 - val_accuracy: 0.5312\n",
            "Epoch 292/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.9149 - accuracy: 0.6667 - val_loss: 1.4168 - val_accuracy: 0.5312\n",
            "Epoch 293/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.9118 - accuracy: 0.6632 - val_loss: 1.4166 - val_accuracy: 0.5312\n",
            "Epoch 294/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.9113 - accuracy: 0.6736 - val_loss: 1.4169 - val_accuracy: 0.5417\n",
            "Epoch 295/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.9098 - accuracy: 0.6632 - val_loss: 1.4211 - val_accuracy: 0.5208\n",
            "Epoch 296/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.9089 - accuracy: 0.6597 - val_loss: 1.4215 - val_accuracy: 0.5312\n",
            "Epoch 297/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.9065 - accuracy: 0.6806 - val_loss: 1.4206 - val_accuracy: 0.5312\n",
            "Epoch 298/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.9059 - accuracy: 0.6562 - val_loss: 1.4246 - val_accuracy: 0.5312\n",
            "Epoch 299/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.9055 - accuracy: 0.6701 - val_loss: 1.4236 - val_accuracy: 0.5312\n",
            "Epoch 300/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.9043 - accuracy: 0.6597 - val_loss: 1.4299 - val_accuracy: 0.5208\n",
            "Epoch 301/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.9014 - accuracy: 0.6701 - val_loss: 1.4255 - val_accuracy: 0.5208\n",
            "Epoch 302/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.9017 - accuracy: 0.6736 - val_loss: 1.4261 - val_accuracy: 0.5312\n",
            "Epoch 303/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.9007 - accuracy: 0.6701 - val_loss: 1.4284 - val_accuracy: 0.5208\n",
            "Epoch 304/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.8975 - accuracy: 0.6528 - val_loss: 1.4320 - val_accuracy: 0.5208\n",
            "Epoch 305/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.8964 - accuracy: 0.6771 - val_loss: 1.4322 - val_accuracy: 0.5104\n",
            "Epoch 306/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.8952 - accuracy: 0.6597 - val_loss: 1.4376 - val_accuracy: 0.5104\n",
            "Epoch 307/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.8950 - accuracy: 0.6667 - val_loss: 1.4411 - val_accuracy: 0.5000\n",
            "Epoch 308/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.8926 - accuracy: 0.6806 - val_loss: 1.4366 - val_accuracy: 0.5104\n",
            "Epoch 309/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.8917 - accuracy: 0.6632 - val_loss: 1.4388 - val_accuracy: 0.5104\n",
            "Epoch 310/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.8893 - accuracy: 0.6840 - val_loss: 1.4384 - val_accuracy: 0.5104\n",
            "Epoch 311/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.8891 - accuracy: 0.6597 - val_loss: 1.4451 - val_accuracy: 0.5000\n",
            "Epoch 312/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.8905 - accuracy: 0.6632 - val_loss: 1.4421 - val_accuracy: 0.5104\n",
            "Epoch 313/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.8857 - accuracy: 0.6701 - val_loss: 1.4451 - val_accuracy: 0.5104\n",
            "Epoch 314/400\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.8835 - accuracy: 0.6736 - val_loss: 1.4431 - val_accuracy: 0.5208\n",
            "Epoch 315/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.8830 - accuracy: 0.6736 - val_loss: 1.4450 - val_accuracy: 0.5104\n",
            "Epoch 316/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.8827 - accuracy: 0.6701 - val_loss: 1.4492 - val_accuracy: 0.5104\n",
            "Epoch 317/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.8800 - accuracy: 0.6840 - val_loss: 1.4499 - val_accuracy: 0.5104\n",
            "Epoch 318/400\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.8813 - accuracy: 0.6771 - val_loss: 1.4521 - val_accuracy: 0.5104\n",
            "Epoch 319/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.8773 - accuracy: 0.6875 - val_loss: 1.4501 - val_accuracy: 0.5104\n",
            "Epoch 320/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.8760 - accuracy: 0.6910 - val_loss: 1.4541 - val_accuracy: 0.5104\n",
            "Epoch 321/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.8753 - accuracy: 0.6806 - val_loss: 1.4560 - val_accuracy: 0.5000\n",
            "Epoch 322/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.8743 - accuracy: 0.6771 - val_loss: 1.4593 - val_accuracy: 0.4896\n",
            "Epoch 323/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.8711 - accuracy: 0.6910 - val_loss: 1.4538 - val_accuracy: 0.5208\n",
            "Epoch 324/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.8703 - accuracy: 0.6736 - val_loss: 1.4586 - val_accuracy: 0.5104\n",
            "Epoch 325/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.8694 - accuracy: 0.6667 - val_loss: 1.4639 - val_accuracy: 0.4896\n",
            "Epoch 326/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.8676 - accuracy: 0.6910 - val_loss: 1.4627 - val_accuracy: 0.5104\n",
            "Epoch 327/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.8656 - accuracy: 0.6840 - val_loss: 1.4600 - val_accuracy: 0.5208\n",
            "Epoch 328/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.8662 - accuracy: 0.6736 - val_loss: 1.4647 - val_accuracy: 0.5208\n",
            "Epoch 329/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.8643 - accuracy: 0.6840 - val_loss: 1.4701 - val_accuracy: 0.4896\n",
            "Epoch 330/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.8629 - accuracy: 0.6771 - val_loss: 1.4671 - val_accuracy: 0.5104\n",
            "Epoch 331/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.8624 - accuracy: 0.6736 - val_loss: 1.4731 - val_accuracy: 0.5000\n",
            "Epoch 332/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.8602 - accuracy: 0.6736 - val_loss: 1.4733 - val_accuracy: 0.5000\n",
            "Epoch 333/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.8591 - accuracy: 0.6771 - val_loss: 1.4726 - val_accuracy: 0.5000\n",
            "Epoch 334/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.8564 - accuracy: 0.6736 - val_loss: 1.4744 - val_accuracy: 0.5000\n",
            "Epoch 335/400\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.8546 - accuracy: 0.6736 - val_loss: 1.4756 - val_accuracy: 0.5000\n",
            "Epoch 336/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.8553 - accuracy: 0.6806 - val_loss: 1.4746 - val_accuracy: 0.5000\n",
            "Epoch 337/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.8511 - accuracy: 0.6910 - val_loss: 1.4755 - val_accuracy: 0.5000\n",
            "Epoch 338/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.8508 - accuracy: 0.6701 - val_loss: 1.4811 - val_accuracy: 0.5000\n",
            "Epoch 339/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.8503 - accuracy: 0.6806 - val_loss: 1.4865 - val_accuracy: 0.4896\n",
            "Epoch 340/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.8477 - accuracy: 0.6771 - val_loss: 1.4883 - val_accuracy: 0.4896\n",
            "Epoch 341/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.8477 - accuracy: 0.6806 - val_loss: 1.4836 - val_accuracy: 0.5000\n",
            "Epoch 342/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.8458 - accuracy: 0.6771 - val_loss: 1.4865 - val_accuracy: 0.5000\n",
            "Epoch 343/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.8433 - accuracy: 0.6840 - val_loss: 1.4876 - val_accuracy: 0.5000\n",
            "Epoch 344/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.8416 - accuracy: 0.6840 - val_loss: 1.4872 - val_accuracy: 0.5000\n",
            "Epoch 345/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.8434 - accuracy: 0.6771 - val_loss: 1.4884 - val_accuracy: 0.5000\n",
            "Epoch 346/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.8384 - accuracy: 0.6806 - val_loss: 1.4949 - val_accuracy: 0.5000\n",
            "Epoch 347/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.8384 - accuracy: 0.6910 - val_loss: 1.4921 - val_accuracy: 0.5000\n",
            "Epoch 348/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.8363 - accuracy: 0.6840 - val_loss: 1.4980 - val_accuracy: 0.5000\n",
            "Epoch 349/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.8345 - accuracy: 0.6840 - val_loss: 1.4993 - val_accuracy: 0.5000\n",
            "Epoch 350/400\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.8321 - accuracy: 0.6840 - val_loss: 1.4964 - val_accuracy: 0.5000\n",
            "Epoch 351/400\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.8316 - accuracy: 0.6771 - val_loss: 1.4981 - val_accuracy: 0.5000\n",
            "Epoch 352/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.8296 - accuracy: 0.6979 - val_loss: 1.5012 - val_accuracy: 0.5000\n",
            "Epoch 353/400\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.8266 - accuracy: 0.6910 - val_loss: 1.4969 - val_accuracy: 0.5104\n",
            "Epoch 354/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.8302 - accuracy: 0.6840 - val_loss: 1.5004 - val_accuracy: 0.5000\n",
            "Epoch 355/400\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.8265 - accuracy: 0.6910 - val_loss: 1.5041 - val_accuracy: 0.5000\n",
            "Epoch 356/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.8241 - accuracy: 0.6806 - val_loss: 1.5082 - val_accuracy: 0.5000\n",
            "Epoch 357/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.8231 - accuracy: 0.6944 - val_loss: 1.5069 - val_accuracy: 0.5000\n",
            "Epoch 358/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.8207 - accuracy: 0.6840 - val_loss: 1.5063 - val_accuracy: 0.5104\n",
            "Epoch 359/400\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.8204 - accuracy: 0.6771 - val_loss: 1.5140 - val_accuracy: 0.4896\n",
            "Epoch 360/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.8203 - accuracy: 0.6771 - val_loss: 1.5137 - val_accuracy: 0.5000\n",
            "Epoch 361/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.8175 - accuracy: 0.6875 - val_loss: 1.5194 - val_accuracy: 0.4792\n",
            "Epoch 362/400\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.8166 - accuracy: 0.6910 - val_loss: 1.5169 - val_accuracy: 0.5000\n",
            "Epoch 363/400\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.8133 - accuracy: 0.6944 - val_loss: 1.5129 - val_accuracy: 0.5104\n",
            "Epoch 364/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.8121 - accuracy: 0.6944 - val_loss: 1.5260 - val_accuracy: 0.4792\n",
            "Epoch 365/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.8164 - accuracy: 0.6944 - val_loss: 1.5206 - val_accuracy: 0.5000\n",
            "Epoch 366/400\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.8097 - accuracy: 0.6944 - val_loss: 1.5207 - val_accuracy: 0.5104\n",
            "Epoch 367/400\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.8094 - accuracy: 0.6875 - val_loss: 1.5245 - val_accuracy: 0.5000\n",
            "Epoch 368/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.8083 - accuracy: 0.6910 - val_loss: 1.5275 - val_accuracy: 0.4896\n",
            "Epoch 369/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.8070 - accuracy: 0.6910 - val_loss: 1.5282 - val_accuracy: 0.5000\n",
            "Epoch 370/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.8043 - accuracy: 0.6875 - val_loss: 1.5262 - val_accuracy: 0.5104\n",
            "Epoch 371/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.8043 - accuracy: 0.6875 - val_loss: 1.5323 - val_accuracy: 0.4896\n",
            "Epoch 372/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.8014 - accuracy: 0.7014 - val_loss: 1.5353 - val_accuracy: 0.4896\n",
            "Epoch 373/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.8006 - accuracy: 0.6875 - val_loss: 1.5312 - val_accuracy: 0.5104\n",
            "Epoch 374/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.8011 - accuracy: 0.6806 - val_loss: 1.5320 - val_accuracy: 0.5000\n",
            "Epoch 375/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.7983 - accuracy: 0.6840 - val_loss: 1.5361 - val_accuracy: 0.5000\n",
            "Epoch 376/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.7955 - accuracy: 0.7118 - val_loss: 1.5393 - val_accuracy: 0.4896\n",
            "Epoch 377/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.7972 - accuracy: 0.6979 - val_loss: 1.5439 - val_accuracy: 0.4896\n",
            "Epoch 378/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.7925 - accuracy: 0.7014 - val_loss: 1.5419 - val_accuracy: 0.4896\n",
            "Epoch 379/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.7926 - accuracy: 0.7083 - val_loss: 1.5465 - val_accuracy: 0.4896\n",
            "Epoch 380/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.7906 - accuracy: 0.7049 - val_loss: 1.5447 - val_accuracy: 0.4896\n",
            "Epoch 381/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.7903 - accuracy: 0.7049 - val_loss: 1.5473 - val_accuracy: 0.4896\n",
            "Epoch 382/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.7873 - accuracy: 0.6979 - val_loss: 1.5524 - val_accuracy: 0.4896\n",
            "Epoch 383/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.7871 - accuracy: 0.7118 - val_loss: 1.5519 - val_accuracy: 0.4896\n",
            "Epoch 384/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.7831 - accuracy: 0.7083 - val_loss: 1.5511 - val_accuracy: 0.4896\n",
            "Epoch 385/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.7825 - accuracy: 0.6944 - val_loss: 1.5617 - val_accuracy: 0.4688\n",
            "Epoch 386/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.7807 - accuracy: 0.7118 - val_loss: 1.5590 - val_accuracy: 0.4896\n",
            "Epoch 387/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.7822 - accuracy: 0.7083 - val_loss: 1.5605 - val_accuracy: 0.4896\n",
            "Epoch 388/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.7801 - accuracy: 0.7014 - val_loss: 1.5657 - val_accuracy: 0.4792\n",
            "Epoch 389/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.7780 - accuracy: 0.7049 - val_loss: 1.5635 - val_accuracy: 0.4896\n",
            "Epoch 390/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.7754 - accuracy: 0.7014 - val_loss: 1.5637 - val_accuracy: 0.4896\n",
            "Epoch 391/400\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.7752 - accuracy: 0.7049 - val_loss: 1.5696 - val_accuracy: 0.4896\n",
            "Epoch 392/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.7723 - accuracy: 0.7049 - val_loss: 1.5650 - val_accuracy: 0.4896\n",
            "Epoch 393/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.7717 - accuracy: 0.7014 - val_loss: 1.5699 - val_accuracy: 0.4896\n",
            "Epoch 394/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.7681 - accuracy: 0.7188 - val_loss: 1.5706 - val_accuracy: 0.4896\n",
            "Epoch 395/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.7705 - accuracy: 0.7188 - val_loss: 1.5734 - val_accuracy: 0.4896\n",
            "Epoch 396/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.7689 - accuracy: 0.7083 - val_loss: 1.5743 - val_accuracy: 0.4896\n",
            "Epoch 397/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.7663 - accuracy: 0.7188 - val_loss: 1.5756 - val_accuracy: 0.4896\n",
            "Epoch 398/400\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.7626 - accuracy: 0.7188 - val_loss: 1.5749 - val_accuracy: 0.5104\n",
            "Epoch 399/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.7628 - accuracy: 0.7257 - val_loss: 1.5810 - val_accuracy: 0.4896\n",
            "Epoch 400/400\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.7627 - accuracy: 0.7222 - val_loss: 1.5846 - val_accuracy: 0.4896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0,3)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "WJzu8cG2sX-S",
        "outputId": "1e524e4a-7700-402c-cd67-15abb52486d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retraining the Model"
      ],
      "metadata": {
        "id": "dhzqUrraOtUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We saw that the validation loss began to increase at around training step 120. Training the network additional times would overfit the network to the training data. Thus, we retrain the model, this time only training it around 120 times to avoid overfitting."
      ],
      "metadata": {
        "id": "tOfky7x-KDZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "t.random.set_seed(0)\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(18, input_dim=18, activation='relu'))\n",
        "model.add(keras.layers.Dense(36, activation = \"relu\"))\n",
        "model.add(keras.layers.Dense(18, activation = \"relu\"))\n",
        "model.add(keras.layers.Dense(8, activation = \"softmax\"))\n",
        "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"sgd\", metrics = [\"accuracy\"])\n",
        "history = model.fit(trainingX, trainingY, epochs = 120, validation_data = (validationX, validationY))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqTiT7DPOxuG",
        "outputId": "5fbce7a4-2028-410c-a334-f407ed96de66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/120\n",
            "9/9 [==============================] - 1s 32ms/step - loss: 1.9802 - accuracy: 0.3681 - val_loss: 1.9105 - val_accuracy: 0.5417\n",
            "Epoch 2/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.9290 - accuracy: 0.4201 - val_loss: 1.8575 - val_accuracy: 0.5104\n",
            "Epoch 3/120\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.8814 - accuracy: 0.4306 - val_loss: 1.8077 - val_accuracy: 0.5312\n",
            "Epoch 4/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.8386 - accuracy: 0.4306 - val_loss: 1.7623 - val_accuracy: 0.5417\n",
            "Epoch 5/120\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.8005 - accuracy: 0.4340 - val_loss: 1.7218 - val_accuracy: 0.5417\n",
            "Epoch 6/120\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.7677 - accuracy: 0.4410 - val_loss: 1.6870 - val_accuracy: 0.5417\n",
            "Epoch 7/120\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.7398 - accuracy: 0.4410 - val_loss: 1.6572 - val_accuracy: 0.5417\n",
            "Epoch 8/120\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.7156 - accuracy: 0.4410 - val_loss: 1.6317 - val_accuracy: 0.5417\n",
            "Epoch 9/120\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.6954 - accuracy: 0.4410 - val_loss: 1.6093 - val_accuracy: 0.5417\n",
            "Epoch 10/120\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.6774 - accuracy: 0.4410 - val_loss: 1.5902 - val_accuracy: 0.5417\n",
            "Epoch 11/120\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.6612 - accuracy: 0.4410 - val_loss: 1.5734 - val_accuracy: 0.5417\n",
            "Epoch 12/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.6460 - accuracy: 0.4410 - val_loss: 1.5590 - val_accuracy: 0.5417\n",
            "Epoch 13/120\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.6327 - accuracy: 0.4410 - val_loss: 1.5462 - val_accuracy: 0.5417\n",
            "Epoch 14/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.6208 - accuracy: 0.4410 - val_loss: 1.5340 - val_accuracy: 0.5417\n",
            "Epoch 15/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.6092 - accuracy: 0.4410 - val_loss: 1.5237 - val_accuracy: 0.5417\n",
            "Epoch 16/120\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.5967 - accuracy: 0.4410 - val_loss: 1.5141 - val_accuracy: 0.5417\n",
            "Epoch 17/120\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.5857 - accuracy: 0.4410 - val_loss: 1.5052 - val_accuracy: 0.5417\n",
            "Epoch 18/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.5757 - accuracy: 0.4410 - val_loss: 1.4966 - val_accuracy: 0.5417\n",
            "Epoch 19/120\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.5649 - accuracy: 0.4410 - val_loss: 1.4879 - val_accuracy: 0.5417\n",
            "Epoch 20/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.5559 - accuracy: 0.4410 - val_loss: 1.4805 - val_accuracy: 0.5417\n",
            "Epoch 21/120\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.5465 - accuracy: 0.4410 - val_loss: 1.4733 - val_accuracy: 0.5417\n",
            "Epoch 22/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.5368 - accuracy: 0.4410 - val_loss: 1.4663 - val_accuracy: 0.5417\n",
            "Epoch 23/120\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.5278 - accuracy: 0.4410 - val_loss: 1.4598 - val_accuracy: 0.5417\n",
            "Epoch 24/120\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.5186 - accuracy: 0.4410 - val_loss: 1.4540 - val_accuracy: 0.5417\n",
            "Epoch 25/120\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.5103 - accuracy: 0.4410 - val_loss: 1.4474 - val_accuracy: 0.5417\n",
            "Epoch 26/120\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.5019 - accuracy: 0.4410 - val_loss: 1.4416 - val_accuracy: 0.5312\n",
            "Epoch 27/120\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.4942 - accuracy: 0.4410 - val_loss: 1.4360 - val_accuracy: 0.5312\n",
            "Epoch 28/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.4868 - accuracy: 0.4410 - val_loss: 1.4302 - val_accuracy: 0.5312\n",
            "Epoch 29/120\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.4792 - accuracy: 0.4444 - val_loss: 1.4257 - val_accuracy: 0.5312\n",
            "Epoch 30/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.4721 - accuracy: 0.4514 - val_loss: 1.4210 - val_accuracy: 0.5312\n",
            "Epoch 31/120\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.4652 - accuracy: 0.4618 - val_loss: 1.4172 - val_accuracy: 0.5312\n",
            "Epoch 32/120\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.4577 - accuracy: 0.4618 - val_loss: 1.4135 - val_accuracy: 0.5312\n",
            "Epoch 33/120\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.4517 - accuracy: 0.4653 - val_loss: 1.4105 - val_accuracy: 0.5417\n",
            "Epoch 34/120\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1.4451 - accuracy: 0.4618 - val_loss: 1.4076 - val_accuracy: 0.5417\n",
            "Epoch 35/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.4382 - accuracy: 0.4688 - val_loss: 1.4038 - val_accuracy: 0.5417\n",
            "Epoch 36/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.4319 - accuracy: 0.4688 - val_loss: 1.4012 - val_accuracy: 0.5417\n",
            "Epoch 37/120\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.4252 - accuracy: 0.4688 - val_loss: 1.3982 - val_accuracy: 0.5417\n",
            "Epoch 38/120\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.4196 - accuracy: 0.4722 - val_loss: 1.3949 - val_accuracy: 0.5417\n",
            "Epoch 39/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.4138 - accuracy: 0.4722 - val_loss: 1.3923 - val_accuracy: 0.5417\n",
            "Epoch 40/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.4086 - accuracy: 0.4722 - val_loss: 1.3896 - val_accuracy: 0.5417\n",
            "Epoch 41/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.4026 - accuracy: 0.4722 - val_loss: 1.3882 - val_accuracy: 0.5521\n",
            "Epoch 42/120\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.3971 - accuracy: 0.4757 - val_loss: 1.3864 - val_accuracy: 0.5521\n",
            "Epoch 43/120\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.3918 - accuracy: 0.4757 - val_loss: 1.3842 - val_accuracy: 0.5417\n",
            "Epoch 44/120\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.3864 - accuracy: 0.4757 - val_loss: 1.3816 - val_accuracy: 0.5417\n",
            "Epoch 45/120\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.3824 - accuracy: 0.4722 - val_loss: 1.3786 - val_accuracy: 0.5417\n",
            "Epoch 46/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.3766 - accuracy: 0.4757 - val_loss: 1.3762 - val_accuracy: 0.5417\n",
            "Epoch 47/120\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.3719 - accuracy: 0.4757 - val_loss: 1.3736 - val_accuracy: 0.5521\n",
            "Epoch 48/120\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.3674 - accuracy: 0.4792 - val_loss: 1.3717 - val_accuracy: 0.5521\n",
            "Epoch 49/120\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.3628 - accuracy: 0.4826 - val_loss: 1.3704 - val_accuracy: 0.5625\n",
            "Epoch 50/120\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.3598 - accuracy: 0.4792 - val_loss: 1.3678 - val_accuracy: 0.5625\n",
            "Epoch 51/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.3540 - accuracy: 0.4792 - val_loss: 1.3664 - val_accuracy: 0.5625\n",
            "Epoch 52/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.3496 - accuracy: 0.4792 - val_loss: 1.3649 - val_accuracy: 0.5625\n",
            "Epoch 53/120\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.3471 - accuracy: 0.4792 - val_loss: 1.3629 - val_accuracy: 0.5625\n",
            "Epoch 54/120\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.3421 - accuracy: 0.4757 - val_loss: 1.3620 - val_accuracy: 0.5625\n",
            "Epoch 55/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.3374 - accuracy: 0.4792 - val_loss: 1.3599 - val_accuracy: 0.5625\n",
            "Epoch 56/120\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.3342 - accuracy: 0.4792 - val_loss: 1.3583 - val_accuracy: 0.5625\n",
            "Epoch 57/120\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.3305 - accuracy: 0.4792 - val_loss: 1.3576 - val_accuracy: 0.5625\n",
            "Epoch 58/120\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.3265 - accuracy: 0.4757 - val_loss: 1.3561 - val_accuracy: 0.5625\n",
            "Epoch 59/120\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.3232 - accuracy: 0.4757 - val_loss: 1.3550 - val_accuracy: 0.5625\n",
            "Epoch 60/120\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.3193 - accuracy: 0.4792 - val_loss: 1.3543 - val_accuracy: 0.5625\n",
            "Epoch 61/120\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.3152 - accuracy: 0.4722 - val_loss: 1.3515 - val_accuracy: 0.5625\n",
            "Epoch 62/120\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.3121 - accuracy: 0.4722 - val_loss: 1.3500 - val_accuracy: 0.5521\n",
            "Epoch 63/120\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.3089 - accuracy: 0.4757 - val_loss: 1.3473 - val_accuracy: 0.5521\n",
            "Epoch 64/120\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.3068 - accuracy: 0.4722 - val_loss: 1.3466 - val_accuracy: 0.5417\n",
            "Epoch 65/120\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.3028 - accuracy: 0.4792 - val_loss: 1.3453 - val_accuracy: 0.5417\n",
            "Epoch 66/120\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.3010 - accuracy: 0.4792 - val_loss: 1.3420 - val_accuracy: 0.5417\n",
            "Epoch 67/120\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.2967 - accuracy: 0.4757 - val_loss: 1.3424 - val_accuracy: 0.5417\n",
            "Epoch 68/120\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1.2943 - accuracy: 0.4757 - val_loss: 1.3418 - val_accuracy: 0.5312\n",
            "Epoch 69/120\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.2914 - accuracy: 0.4792 - val_loss: 1.3403 - val_accuracy: 0.5312\n",
            "Epoch 70/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.2881 - accuracy: 0.4826 - val_loss: 1.3391 - val_accuracy: 0.5312\n",
            "Epoch 71/120\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.2861 - accuracy: 0.4757 - val_loss: 1.3392 - val_accuracy: 0.5312\n",
            "Epoch 72/120\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.2835 - accuracy: 0.4792 - val_loss: 1.3373 - val_accuracy: 0.5208\n",
            "Epoch 73/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.2810 - accuracy: 0.4757 - val_loss: 1.3364 - val_accuracy: 0.5208\n",
            "Epoch 74/120\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.2785 - accuracy: 0.4861 - val_loss: 1.3363 - val_accuracy: 0.5208\n",
            "Epoch 75/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.2761 - accuracy: 0.4861 - val_loss: 1.3332 - val_accuracy: 0.5208\n",
            "Epoch 76/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.2723 - accuracy: 0.4896 - val_loss: 1.3324 - val_accuracy: 0.5208\n",
            "Epoch 77/120\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.2704 - accuracy: 0.4826 - val_loss: 1.3299 - val_accuracy: 0.5208\n",
            "Epoch 78/120\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.2675 - accuracy: 0.4826 - val_loss: 1.3295 - val_accuracy: 0.5208\n",
            "Epoch 79/120\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.2659 - accuracy: 0.4792 - val_loss: 1.3289 - val_accuracy: 0.5208\n",
            "Epoch 80/120\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.2625 - accuracy: 0.4792 - val_loss: 1.3294 - val_accuracy: 0.5208\n",
            "Epoch 81/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.2601 - accuracy: 0.4826 - val_loss: 1.3260 - val_accuracy: 0.5208\n",
            "Epoch 82/120\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.2586 - accuracy: 0.4861 - val_loss: 1.3246 - val_accuracy: 0.5208\n",
            "Epoch 83/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.2572 - accuracy: 0.4861 - val_loss: 1.3255 - val_accuracy: 0.5208\n",
            "Epoch 84/120\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.2537 - accuracy: 0.4826 - val_loss: 1.3241 - val_accuracy: 0.5104\n",
            "Epoch 85/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.2517 - accuracy: 0.4826 - val_loss: 1.3221 - val_accuracy: 0.5104\n",
            "Epoch 86/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.2498 - accuracy: 0.4792 - val_loss: 1.3220 - val_accuracy: 0.5000\n",
            "Epoch 87/120\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.2469 - accuracy: 0.4861 - val_loss: 1.3208 - val_accuracy: 0.5000\n",
            "Epoch 88/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.2464 - accuracy: 0.4861 - val_loss: 1.3181 - val_accuracy: 0.5000\n",
            "Epoch 89/120\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.2440 - accuracy: 0.4757 - val_loss: 1.3205 - val_accuracy: 0.5000\n",
            "Epoch 90/120\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.2418 - accuracy: 0.4826 - val_loss: 1.3197 - val_accuracy: 0.5000\n",
            "Epoch 91/120\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.2381 - accuracy: 0.4826 - val_loss: 1.3211 - val_accuracy: 0.5000\n",
            "Epoch 92/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.2368 - accuracy: 0.4792 - val_loss: 1.3194 - val_accuracy: 0.5000\n",
            "Epoch 93/120\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.2338 - accuracy: 0.4861 - val_loss: 1.3169 - val_accuracy: 0.5000\n",
            "Epoch 94/120\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.2324 - accuracy: 0.4861 - val_loss: 1.3168 - val_accuracy: 0.5000\n",
            "Epoch 95/120\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.2294 - accuracy: 0.4965 - val_loss: 1.3169 - val_accuracy: 0.5000\n",
            "Epoch 96/120\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.2284 - accuracy: 0.4896 - val_loss: 1.3144 - val_accuracy: 0.5000\n",
            "Epoch 97/120\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.2260 - accuracy: 0.4931 - val_loss: 1.3141 - val_accuracy: 0.5000\n",
            "Epoch 98/120\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.2239 - accuracy: 0.4965 - val_loss: 1.3134 - val_accuracy: 0.5000\n",
            "Epoch 99/120\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.2219 - accuracy: 0.4965 - val_loss: 1.3117 - val_accuracy: 0.5000\n",
            "Epoch 100/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.2196 - accuracy: 0.4965 - val_loss: 1.3112 - val_accuracy: 0.5000\n",
            "Epoch 101/120\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.2181 - accuracy: 0.4931 - val_loss: 1.3104 - val_accuracy: 0.5000\n",
            "Epoch 102/120\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.2169 - accuracy: 0.4896 - val_loss: 1.3110 - val_accuracy: 0.5000\n",
            "Epoch 103/120\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.2149 - accuracy: 0.4965 - val_loss: 1.3088 - val_accuracy: 0.5000\n",
            "Epoch 104/120\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.2118 - accuracy: 0.4965 - val_loss: 1.3100 - val_accuracy: 0.5000\n",
            "Epoch 105/120\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.2105 - accuracy: 0.4965 - val_loss: 1.3099 - val_accuracy: 0.4896\n",
            "Epoch 106/120\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.2099 - accuracy: 0.5000 - val_loss: 1.3088 - val_accuracy: 0.4896\n",
            "Epoch 107/120\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 1.2068 - accuracy: 0.5035 - val_loss: 1.3100 - val_accuracy: 0.4896\n",
            "Epoch 108/120\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.2048 - accuracy: 0.5069 - val_loss: 1.3068 - val_accuracy: 0.5000\n",
            "Epoch 109/120\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.2042 - accuracy: 0.5035 - val_loss: 1.3053 - val_accuracy: 0.4896\n",
            "Epoch 110/120\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.2021 - accuracy: 0.5069 - val_loss: 1.3071 - val_accuracy: 0.4792\n",
            "Epoch 111/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.1993 - accuracy: 0.5069 - val_loss: 1.3063 - val_accuracy: 0.4792\n",
            "Epoch 112/120\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.1984 - accuracy: 0.5035 - val_loss: 1.3086 - val_accuracy: 0.4688\n",
            "Epoch 113/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.1955 - accuracy: 0.5174 - val_loss: 1.3051 - val_accuracy: 0.4792\n",
            "Epoch 114/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.1948 - accuracy: 0.5104 - val_loss: 1.3044 - val_accuracy: 0.4792\n",
            "Epoch 115/120\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.1941 - accuracy: 0.5069 - val_loss: 1.3047 - val_accuracy: 0.4792\n",
            "Epoch 116/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.1934 - accuracy: 0.5104 - val_loss: 1.3064 - val_accuracy: 0.4583\n",
            "Epoch 117/120\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1.1892 - accuracy: 0.5139 - val_loss: 1.3053 - val_accuracy: 0.4688\n",
            "Epoch 118/120\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.1875 - accuracy: 0.5174 - val_loss: 1.3040 - val_accuracy: 0.4792\n",
            "Epoch 119/120\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.1852 - accuracy: 0.5139 - val_loss: 1.3066 - val_accuracy: 0.4688\n",
            "Epoch 120/120\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1.1841 - accuracy: 0.5069 - val_loss: 1.3063 - val_accuracy: 0.4688\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0,3)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "OU15ZHWVPhen",
        "outputId": "1a850fb3-c5e7-4679-c656-2d8bf3e39b25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After approximately 40 training steps, the network achieves a validation accuracy of just above 50%. This indicates that the network accurately categorizes teams in the validation set based on the round of March Madness in which they are eliminated approximately 60% of the time. The network does not improve much after this point."
      ],
      "metadata": {
        "id": "Y4bmv3EmIlD5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting Using Current Season Data"
      ],
      "metadata": {
        "id": "wcgUjMILItKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the network above, we predict the tournament results of the 2023 Tournament teams. As a refresher the data of these teams is presented below."
      ],
      "metadata": {
        "id": "HK-xmws4Ifx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "currentTournamentTeams.drop([\"POSTSEASON\"], axis =1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "VV_vTXDFJi1S",
        "outputId": "75ddd260-0677-4b64-b99f-035572e21c43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    index                    TEAM  CONF   G   W  ADJOE  ADJDE  BARTHAG  EFG_O  \\\n",
              "0     480                 Houston  Amer  34  31  117.1   88.0   0.9638   52.7   \n",
              "1     481                 Alabama   SEC  34  29  115.4   88.3   0.9557   52.7   \n",
              "2     482               Creighton    BE  33  21  113.7   92.9   0.9112   54.3   \n",
              "3     483             Connecticut    BE  33  25  118.8   92.5   0.9466   53.5   \n",
              "4     484           San Diego St.   MWC  36  31  110.8   88.8   0.9276   49.6   \n",
              "..    ...                     ...   ...  ..  ..    ...    ...      ...    ...   \n",
              "62    542           UNC Asheville  Bsth  32  25  101.2  102.4   0.4668   53.9   \n",
              "63    543  Texas A&M Corpus Chris  SInd  30  20  104.3  106.0   0.4540   50.9   \n",
              "64    544                 Howard   MEAC  31  19  100.9  105.7   0.3695   52.0   \n",
              "65    545  Southeast Missouri St.   OVC  34  17   98.6  106.8   0.2856   50.1   \n",
              "66    546     Fairleigh Dickinson   NEC  33  18  105.0  116.4   0.2330   51.7   \n",
              "\n",
              "    EFG_D  ...  2P_O  2P_D  3P_O  3P_D  ADJ_T   WAB  SEED    YEAR  \\\n",
              "0    42.5  ...  53.2  43.1  34.5  27.8   64.0   8.0   1.0  2023.0   \n",
              "1    41.5  ...  54.6  41.2  33.8  28.1   73.5  10.2   1.0  2023.0   \n",
              "2    47.3  ...  54.5  45.6  36.0  34.1   69.9  10.0   6.0  2023.0   \n",
              "3    45.5  ...  53.4  45.8  35.7  30.0   67.7   4.9   4.0  2023.0   \n",
              "4    46.3  ...  49.0  49.4  33.9  27.8   65.7   5.7   5.0  2023.0   \n",
              "..    ...  ...   ...   ...   ...   ...    ...   ...   ...     ...   \n",
              "62   48.0  ...  51.4  49.1  38.8  30.5   69.1  -1.9  16.0  2023.0   \n",
              "63   52.4  ...  48.9  54.3  36.5  33.2   69.8  -6.0  16.0  2023.0   \n",
              "64   50.6  ...  49.9  50.8  37.2  33.6   69.7  -7.3  16.0  2023.0   \n",
              "65   51.3  ...  50.3  50.7  33.2  35.0   72.8 -11.7  16.0  2023.0   \n",
              "66   55.4  ...  51.5  56.1  34.7  36.2   69.4 -12.2  16.0  2023.0   \n",
              "\n",
              "    PostseasonNums  CONFB  \n",
              "0                8      4  \n",
              "1                8     30  \n",
              "2                8      9  \n",
              "3                8      9  \n",
              "4                8     23  \n",
              "..             ...    ...  \n",
              "62               8     14  \n",
              "63               8     31  \n",
              "64               8     21  \n",
              "65               8     25  \n",
              "66               8     24  \n",
              "\n",
              "[67 rows x 26 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0bc36aee-d1e9-4d7b-869e-d22b907e69bd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>TEAM</th>\n",
              "      <th>CONF</th>\n",
              "      <th>G</th>\n",
              "      <th>W</th>\n",
              "      <th>ADJOE</th>\n",
              "      <th>ADJDE</th>\n",
              "      <th>BARTHAG</th>\n",
              "      <th>EFG_O</th>\n",
              "      <th>EFG_D</th>\n",
              "      <th>...</th>\n",
              "      <th>2P_O</th>\n",
              "      <th>2P_D</th>\n",
              "      <th>3P_O</th>\n",
              "      <th>3P_D</th>\n",
              "      <th>ADJ_T</th>\n",
              "      <th>WAB</th>\n",
              "      <th>SEED</th>\n",
              "      <th>YEAR</th>\n",
              "      <th>PostseasonNums</th>\n",
              "      <th>CONFB</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>480</td>\n",
              "      <td>Houston</td>\n",
              "      <td>Amer</td>\n",
              "      <td>34</td>\n",
              "      <td>31</td>\n",
              "      <td>117.1</td>\n",
              "      <td>88.0</td>\n",
              "      <td>0.9638</td>\n",
              "      <td>52.7</td>\n",
              "      <td>42.5</td>\n",
              "      <td>...</td>\n",
              "      <td>53.2</td>\n",
              "      <td>43.1</td>\n",
              "      <td>34.5</td>\n",
              "      <td>27.8</td>\n",
              "      <td>64.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2023.0</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>481</td>\n",
              "      <td>Alabama</td>\n",
              "      <td>SEC</td>\n",
              "      <td>34</td>\n",
              "      <td>29</td>\n",
              "      <td>115.4</td>\n",
              "      <td>88.3</td>\n",
              "      <td>0.9557</td>\n",
              "      <td>52.7</td>\n",
              "      <td>41.5</td>\n",
              "      <td>...</td>\n",
              "      <td>54.6</td>\n",
              "      <td>41.2</td>\n",
              "      <td>33.8</td>\n",
              "      <td>28.1</td>\n",
              "      <td>73.5</td>\n",
              "      <td>10.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2023.0</td>\n",
              "      <td>8</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>482</td>\n",
              "      <td>Creighton</td>\n",
              "      <td>BE</td>\n",
              "      <td>33</td>\n",
              "      <td>21</td>\n",
              "      <td>113.7</td>\n",
              "      <td>92.9</td>\n",
              "      <td>0.9112</td>\n",
              "      <td>54.3</td>\n",
              "      <td>47.3</td>\n",
              "      <td>...</td>\n",
              "      <td>54.5</td>\n",
              "      <td>45.6</td>\n",
              "      <td>36.0</td>\n",
              "      <td>34.1</td>\n",
              "      <td>69.9</td>\n",
              "      <td>10.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2023.0</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>483</td>\n",
              "      <td>Connecticut</td>\n",
              "      <td>BE</td>\n",
              "      <td>33</td>\n",
              "      <td>25</td>\n",
              "      <td>118.8</td>\n",
              "      <td>92.5</td>\n",
              "      <td>0.9466</td>\n",
              "      <td>53.5</td>\n",
              "      <td>45.5</td>\n",
              "      <td>...</td>\n",
              "      <td>53.4</td>\n",
              "      <td>45.8</td>\n",
              "      <td>35.7</td>\n",
              "      <td>30.0</td>\n",
              "      <td>67.7</td>\n",
              "      <td>4.9</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2023.0</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>484</td>\n",
              "      <td>San Diego St.</td>\n",
              "      <td>MWC</td>\n",
              "      <td>36</td>\n",
              "      <td>31</td>\n",
              "      <td>110.8</td>\n",
              "      <td>88.8</td>\n",
              "      <td>0.9276</td>\n",
              "      <td>49.6</td>\n",
              "      <td>46.3</td>\n",
              "      <td>...</td>\n",
              "      <td>49.0</td>\n",
              "      <td>49.4</td>\n",
              "      <td>33.9</td>\n",
              "      <td>27.8</td>\n",
              "      <td>65.7</td>\n",
              "      <td>5.7</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2023.0</td>\n",
              "      <td>8</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>542</td>\n",
              "      <td>UNC Asheville</td>\n",
              "      <td>Bsth</td>\n",
              "      <td>32</td>\n",
              "      <td>25</td>\n",
              "      <td>101.2</td>\n",
              "      <td>102.4</td>\n",
              "      <td>0.4668</td>\n",
              "      <td>53.9</td>\n",
              "      <td>48.0</td>\n",
              "      <td>...</td>\n",
              "      <td>51.4</td>\n",
              "      <td>49.1</td>\n",
              "      <td>38.8</td>\n",
              "      <td>30.5</td>\n",
              "      <td>69.1</td>\n",
              "      <td>-1.9</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2023.0</td>\n",
              "      <td>8</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>543</td>\n",
              "      <td>Texas A&amp;M Corpus Chris</td>\n",
              "      <td>SInd</td>\n",
              "      <td>30</td>\n",
              "      <td>20</td>\n",
              "      <td>104.3</td>\n",
              "      <td>106.0</td>\n",
              "      <td>0.4540</td>\n",
              "      <td>50.9</td>\n",
              "      <td>52.4</td>\n",
              "      <td>...</td>\n",
              "      <td>48.9</td>\n",
              "      <td>54.3</td>\n",
              "      <td>36.5</td>\n",
              "      <td>33.2</td>\n",
              "      <td>69.8</td>\n",
              "      <td>-6.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2023.0</td>\n",
              "      <td>8</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>544</td>\n",
              "      <td>Howard</td>\n",
              "      <td>MEAC</td>\n",
              "      <td>31</td>\n",
              "      <td>19</td>\n",
              "      <td>100.9</td>\n",
              "      <td>105.7</td>\n",
              "      <td>0.3695</td>\n",
              "      <td>52.0</td>\n",
              "      <td>50.6</td>\n",
              "      <td>...</td>\n",
              "      <td>49.9</td>\n",
              "      <td>50.8</td>\n",
              "      <td>37.2</td>\n",
              "      <td>33.6</td>\n",
              "      <td>69.7</td>\n",
              "      <td>-7.3</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2023.0</td>\n",
              "      <td>8</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>545</td>\n",
              "      <td>Southeast Missouri St.</td>\n",
              "      <td>OVC</td>\n",
              "      <td>34</td>\n",
              "      <td>17</td>\n",
              "      <td>98.6</td>\n",
              "      <td>106.8</td>\n",
              "      <td>0.2856</td>\n",
              "      <td>50.1</td>\n",
              "      <td>51.3</td>\n",
              "      <td>...</td>\n",
              "      <td>50.3</td>\n",
              "      <td>50.7</td>\n",
              "      <td>33.2</td>\n",
              "      <td>35.0</td>\n",
              "      <td>72.8</td>\n",
              "      <td>-11.7</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2023.0</td>\n",
              "      <td>8</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>546</td>\n",
              "      <td>Fairleigh Dickinson</td>\n",
              "      <td>NEC</td>\n",
              "      <td>33</td>\n",
              "      <td>18</td>\n",
              "      <td>105.0</td>\n",
              "      <td>116.4</td>\n",
              "      <td>0.2330</td>\n",
              "      <td>51.7</td>\n",
              "      <td>55.4</td>\n",
              "      <td>...</td>\n",
              "      <td>51.5</td>\n",
              "      <td>56.1</td>\n",
              "      <td>34.7</td>\n",
              "      <td>36.2</td>\n",
              "      <td>69.4</td>\n",
              "      <td>-12.2</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2023.0</td>\n",
              "      <td>8</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>67 rows × 26 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0bc36aee-d1e9-4d7b-869e-d22b907e69bd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0bc36aee-d1e9-4d7b-869e-d22b907e69bd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0bc36aee-d1e9-4d7b-869e-d22b907e69bd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the normalized and refined version of that data"
      ],
      "metadata": {
        "id": "R8V3RtaiSuvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalizedFinalTesting = normalizedFinalTesting.drop([\"index\"],axis =1)"
      ],
      "metadata": {
        "id": "e8jrrmt0Wqqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(normalizedFinalTesting)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "\n",
        "predictionDf = pd.DataFrame((predictions * 100), columns = [\"Champion\", \"Runner Up\", \"Final 4\", \"Elite 8\", \"Sweet 16\", \"Round of 32\", \"Round of 64\", \"Round of 68\"])\n",
        "predictionDf[\"Team\"] = \"\"\n",
        "\n",
        "for row, rowSeries in predictionDf.iterrows():\n",
        "  predictionDf.at[row, \"Team\"] = currentTeams[row]\n",
        "\n",
        "\n",
        "predictionDf"
      ],
      "metadata": {
        "id": "WUTTzKYOLWfn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "0eec090c-ef07-4126-f08b-6ccdbf5545dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Champion  Runner Up   Final 4    Elite 8   Sweet 16  Round of 32  \\\n",
              "0   4.308848e+00   1.770895  4.262580  14.581471  24.333672    33.919193   \n",
              "1   5.353984e+00   1.929349  3.517991  16.237791  26.200974    31.666740   \n",
              "2   3.632420e+00   1.664329  3.030068   8.380038  24.500967    34.123489   \n",
              "3   2.872478e+00   2.049285  3.199738   8.585908  26.403105    33.833546   \n",
              "4   7.385253e-01   0.742036  1.897917   1.631406  12.376484    42.588512   \n",
              "..           ...        ...       ...        ...        ...          ...   \n",
              "62  1.707078e-03   0.042685  0.174508   0.008465   1.625739     7.744429   \n",
              "63  1.897237e-04   0.013020  0.084716   0.001630   0.493378     5.292445   \n",
              "64  7.307514e-06   0.002369  0.012359   0.000080   0.152753     1.983950   \n",
              "65  1.403862e-05   0.001955  0.016787   0.000170   0.238535     3.371224   \n",
              "66  8.856387e-08   0.000169  0.001410   0.000001   0.023472     0.555318   \n",
              "\n",
              "    Round of 64  Round of 68                    Team  \n",
              "0     14.501762     2.321578                 Houston  \n",
              "1     13.669114     1.424062                 Alabama  \n",
              "2     23.162910     1.505769               Creighton  \n",
              "3     20.431007     2.624929             Connecticut  \n",
              "4     36.178391     3.846724           San Diego St.  \n",
              "..          ...          ...                     ...  \n",
              "62    75.520638    14.881827           UNC Asheville  \n",
              "63    60.843647    33.270977  Texas A&M Corpus Chris  \n",
              "64    60.553211    37.295273                 Howard   \n",
              "65    77.720390    18.650928  Southeast Missouri St.  \n",
              "66    46.838234    52.581398     Fairleigh Dickinson  \n",
              "\n",
              "[67 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dc2205ab-b993-4d3e-a915-602958e938d7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Champion</th>\n",
              "      <th>Runner Up</th>\n",
              "      <th>Final 4</th>\n",
              "      <th>Elite 8</th>\n",
              "      <th>Sweet 16</th>\n",
              "      <th>Round of 32</th>\n",
              "      <th>Round of 64</th>\n",
              "      <th>Round of 68</th>\n",
              "      <th>Team</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.308848e+00</td>\n",
              "      <td>1.770895</td>\n",
              "      <td>4.262580</td>\n",
              "      <td>14.581471</td>\n",
              "      <td>24.333672</td>\n",
              "      <td>33.919193</td>\n",
              "      <td>14.501762</td>\n",
              "      <td>2.321578</td>\n",
              "      <td>Houston</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.353984e+00</td>\n",
              "      <td>1.929349</td>\n",
              "      <td>3.517991</td>\n",
              "      <td>16.237791</td>\n",
              "      <td>26.200974</td>\n",
              "      <td>31.666740</td>\n",
              "      <td>13.669114</td>\n",
              "      <td>1.424062</td>\n",
              "      <td>Alabama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.632420e+00</td>\n",
              "      <td>1.664329</td>\n",
              "      <td>3.030068</td>\n",
              "      <td>8.380038</td>\n",
              "      <td>24.500967</td>\n",
              "      <td>34.123489</td>\n",
              "      <td>23.162910</td>\n",
              "      <td>1.505769</td>\n",
              "      <td>Creighton</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.872478e+00</td>\n",
              "      <td>2.049285</td>\n",
              "      <td>3.199738</td>\n",
              "      <td>8.585908</td>\n",
              "      <td>26.403105</td>\n",
              "      <td>33.833546</td>\n",
              "      <td>20.431007</td>\n",
              "      <td>2.624929</td>\n",
              "      <td>Connecticut</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.385253e-01</td>\n",
              "      <td>0.742036</td>\n",
              "      <td>1.897917</td>\n",
              "      <td>1.631406</td>\n",
              "      <td>12.376484</td>\n",
              "      <td>42.588512</td>\n",
              "      <td>36.178391</td>\n",
              "      <td>3.846724</td>\n",
              "      <td>San Diego St.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>1.707078e-03</td>\n",
              "      <td>0.042685</td>\n",
              "      <td>0.174508</td>\n",
              "      <td>0.008465</td>\n",
              "      <td>1.625739</td>\n",
              "      <td>7.744429</td>\n",
              "      <td>75.520638</td>\n",
              "      <td>14.881827</td>\n",
              "      <td>UNC Asheville</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>1.897237e-04</td>\n",
              "      <td>0.013020</td>\n",
              "      <td>0.084716</td>\n",
              "      <td>0.001630</td>\n",
              "      <td>0.493378</td>\n",
              "      <td>5.292445</td>\n",
              "      <td>60.843647</td>\n",
              "      <td>33.270977</td>\n",
              "      <td>Texas A&amp;M Corpus Chris</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>7.307514e-06</td>\n",
              "      <td>0.002369</td>\n",
              "      <td>0.012359</td>\n",
              "      <td>0.000080</td>\n",
              "      <td>0.152753</td>\n",
              "      <td>1.983950</td>\n",
              "      <td>60.553211</td>\n",
              "      <td>37.295273</td>\n",
              "      <td>Howard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>1.403862e-05</td>\n",
              "      <td>0.001955</td>\n",
              "      <td>0.016787</td>\n",
              "      <td>0.000170</td>\n",
              "      <td>0.238535</td>\n",
              "      <td>3.371224</td>\n",
              "      <td>77.720390</td>\n",
              "      <td>18.650928</td>\n",
              "      <td>Southeast Missouri St.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>8.856387e-08</td>\n",
              "      <td>0.000169</td>\n",
              "      <td>0.001410</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>0.023472</td>\n",
              "      <td>0.555318</td>\n",
              "      <td>46.838234</td>\n",
              "      <td>52.581398</td>\n",
              "      <td>Fairleigh Dickinson</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>67 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dc2205ab-b993-4d3e-a915-602958e938d7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dc2205ab-b993-4d3e-a915-602958e938d7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dc2205ab-b993-4d3e-a915-602958e938d7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictionDf[\"Predicted Outcome\"] = \"\"\n",
        "for i in range(0 ,len(predicted_classes)):\n",
        "  predictionDf.at[i, \"Predicted Outcome\"] = classNames[predicted_classes[i]]"
      ],
      "metadata": {
        "id": "2miwvbN8XoUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ten teams given the best chance to win the 2023 March Madness tournament are shown in the table below."
      ],
      "metadata": {
        "id": "uMtH8Bcmhkct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictionDf.sort_values([\"Champion\"], ascending = False).head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "HH3cxIoji1Gr",
        "outputId": "83110e9e-ea95-4043-d22b-12b964f2d7ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Champion  Runner Up   Final 4    Elite 8   Sweet 16  Round of 32  \\\n",
              "20  7.282208   2.380068  4.396314  20.252773  25.479477    27.633974   \n",
              "17  5.872602   1.982180  3.262717  13.318756  19.787130    38.883602   \n",
              "6   5.548048   2.676168  4.335838  14.238079  33.629230    24.676640   \n",
              "40  5.378686   3.715355  5.537641  10.295542  21.690481    29.547552   \n",
              "1   5.353984   1.929349  3.517991  16.237791  26.200974    31.666740   \n",
              "22  5.111673   4.858502  5.474943   8.879020  26.895485    24.218702   \n",
              "32  5.036636   2.836027  5.058550  11.711785  25.115517    28.651455   \n",
              "16  4.910217   1.751992  3.885348  15.981007  28.670534    30.550829   \n",
              "5   4.652938   3.743341  2.972783   8.155657  17.735901    36.394302   \n",
              "0   4.308848   1.770895  4.262580  14.581471  24.333672    33.919193   \n",
              "\n",
              "    Round of 64  Round of 68     Team Predicted Outcome  \n",
              "20    10.758170     1.817013   Kansas       Round of 32  \n",
              "17    15.291934     1.601084     UCLA       Round of 32  \n",
              "6     12.945925     1.950067  Arizona          Sweet 16  \n",
              "40    20.021872     3.812864      USC       Round of 32  \n",
              "1     13.669114     1.424062  Alabama       Round of 32  \n",
              "22    20.135168     4.426504  Memphis          Sweet 16  \n",
              "32    18.526407     3.063615  Indiana       Round of 32  \n",
              "16    12.120568     2.129492    Texas       Round of 32  \n",
              "5     23.900826     2.444260   Purdue       Round of 32  \n",
              "0     14.501762     2.321578  Houston       Round of 32  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-022d5522-bb3d-44e1-b15e-d03f767e232d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Champion</th>\n",
              "      <th>Runner Up</th>\n",
              "      <th>Final 4</th>\n",
              "      <th>Elite 8</th>\n",
              "      <th>Sweet 16</th>\n",
              "      <th>Round of 32</th>\n",
              "      <th>Round of 64</th>\n",
              "      <th>Round of 68</th>\n",
              "      <th>Team</th>\n",
              "      <th>Predicted Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>7.282208</td>\n",
              "      <td>2.380068</td>\n",
              "      <td>4.396314</td>\n",
              "      <td>20.252773</td>\n",
              "      <td>25.479477</td>\n",
              "      <td>27.633974</td>\n",
              "      <td>10.758170</td>\n",
              "      <td>1.817013</td>\n",
              "      <td>Kansas</td>\n",
              "      <td>Round of 32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>5.872602</td>\n",
              "      <td>1.982180</td>\n",
              "      <td>3.262717</td>\n",
              "      <td>13.318756</td>\n",
              "      <td>19.787130</td>\n",
              "      <td>38.883602</td>\n",
              "      <td>15.291934</td>\n",
              "      <td>1.601084</td>\n",
              "      <td>UCLA</td>\n",
              "      <td>Round of 32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5.548048</td>\n",
              "      <td>2.676168</td>\n",
              "      <td>4.335838</td>\n",
              "      <td>14.238079</td>\n",
              "      <td>33.629230</td>\n",
              "      <td>24.676640</td>\n",
              "      <td>12.945925</td>\n",
              "      <td>1.950067</td>\n",
              "      <td>Arizona</td>\n",
              "      <td>Sweet 16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>5.378686</td>\n",
              "      <td>3.715355</td>\n",
              "      <td>5.537641</td>\n",
              "      <td>10.295542</td>\n",
              "      <td>21.690481</td>\n",
              "      <td>29.547552</td>\n",
              "      <td>20.021872</td>\n",
              "      <td>3.812864</td>\n",
              "      <td>USC</td>\n",
              "      <td>Round of 32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.353984</td>\n",
              "      <td>1.929349</td>\n",
              "      <td>3.517991</td>\n",
              "      <td>16.237791</td>\n",
              "      <td>26.200974</td>\n",
              "      <td>31.666740</td>\n",
              "      <td>13.669114</td>\n",
              "      <td>1.424062</td>\n",
              "      <td>Alabama</td>\n",
              "      <td>Round of 32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>5.111673</td>\n",
              "      <td>4.858502</td>\n",
              "      <td>5.474943</td>\n",
              "      <td>8.879020</td>\n",
              "      <td>26.895485</td>\n",
              "      <td>24.218702</td>\n",
              "      <td>20.135168</td>\n",
              "      <td>4.426504</td>\n",
              "      <td>Memphis</td>\n",
              "      <td>Sweet 16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>5.036636</td>\n",
              "      <td>2.836027</td>\n",
              "      <td>5.058550</td>\n",
              "      <td>11.711785</td>\n",
              "      <td>25.115517</td>\n",
              "      <td>28.651455</td>\n",
              "      <td>18.526407</td>\n",
              "      <td>3.063615</td>\n",
              "      <td>Indiana</td>\n",
              "      <td>Round of 32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>4.910217</td>\n",
              "      <td>1.751992</td>\n",
              "      <td>3.885348</td>\n",
              "      <td>15.981007</td>\n",
              "      <td>28.670534</td>\n",
              "      <td>30.550829</td>\n",
              "      <td>12.120568</td>\n",
              "      <td>2.129492</td>\n",
              "      <td>Texas</td>\n",
              "      <td>Round of 32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>4.652938</td>\n",
              "      <td>3.743341</td>\n",
              "      <td>2.972783</td>\n",
              "      <td>8.155657</td>\n",
              "      <td>17.735901</td>\n",
              "      <td>36.394302</td>\n",
              "      <td>23.900826</td>\n",
              "      <td>2.444260</td>\n",
              "      <td>Purdue</td>\n",
              "      <td>Round of 32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.308848</td>\n",
              "      <td>1.770895</td>\n",
              "      <td>4.262580</td>\n",
              "      <td>14.581471</td>\n",
              "      <td>24.333672</td>\n",
              "      <td>33.919193</td>\n",
              "      <td>14.501762</td>\n",
              "      <td>2.321578</td>\n",
              "      <td>Houston</td>\n",
              "      <td>Round of 32</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-022d5522-bb3d-44e1-b15e-d03f767e232d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-022d5522-bb3d-44e1-b15e-d03f767e232d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-022d5522-bb3d-44e1-b15e-d03f767e232d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The predicted tournament outcomes for each 2023 Tournament Team are shown below."
      ],
      "metadata": {
        "id": "pBhJJ5Wkheg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for row, rowSeries in predictionDf.iterrows():\n",
        "  print(rowSeries[\"Team\"], \":\", rowSeries[\"Predicted Outcome\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N85ycVt7ZMyx",
        "outputId": "f7cb8a7a-1c53-4cd8-8b25-94abedaa9d90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Houston : Round of 32\n",
            "Alabama : Round of 32\n",
            "Creighton : Round of 32\n",
            "Connecticut : Round of 32\n",
            "San Diego St. : Round of 32\n",
            "Purdue : Round of 32\n",
            "Arizona : Sweet 16\n",
            "West Virginia : Round of 64\n",
            "Florida Atlantic : Round of 64\n",
            "Princeton : Round of 64\n",
            "Miami FL : Round of 64\n",
            "Virginia : Round of 32\n",
            "TCU : Round of 64\n",
            "Kansas St. : Round of 32\n",
            "Marquette : Sweet 16\n",
            "Gonzaga : Round of 64\n",
            "Texas : Round of 32\n",
            "UCLA : Round of 32\n",
            "Tennessee : Round of 32\n",
            "Saint Mary's : Round of 32\n",
            "Kansas : Round of 32\n",
            "Baylor : Round of 64\n",
            "Memphis : Sweet 16\n",
            "Arkansas : Round of 64\n",
            "Xavier : Round of 64\n",
            "Auburn : Round of 64\n",
            "Iowa St. : Round of 32\n",
            "Duke : Round of 32\n",
            "Texas A&M : Round of 64\n",
            "Utah St. : Round of 64\n",
            "Michigan St. : Round of 64\n",
            "Maryland : Round of 32\n",
            "Indiana : Round of 32\n",
            "Kentucky : Round of 64\n",
            "Boise St. : Round of 64\n",
            "Northwestern : Round of 64\n",
            "Iowa : Round of 64\n",
            "North Carolina St. : Round of 64\n",
            "Penn St. : Round of 64\n",
            "Illinois : Round of 64\n",
            "USC : Round of 32\n",
            "Providence : Round of 64\n",
            "Arizona St. : Round of 64\n",
            "Missouri : Round of 64\n",
            "Mississippi St. : Round of 32\n",
            "Drake : Round of 64\n",
            "Iona : Round of 64\n",
            "VCU : Round of 64\n",
            "College of Charleston : Round of 64\n",
            "Kent St. : Round of 64\n",
            "Nevada : Round of 64\n",
            "Pittsburgh : Round of 64\n",
            "Oral Roberts : Round of 64\n",
            "Louisiana Lafayette : Round of 64\n",
            "Furman : Round of 64\n",
            "Montana St. : Round of 64\n",
            "Vermont : Round of 64\n",
            "Colgate : Round of 64\n",
            "Grand Canyon : Round of 64\n",
            "Kennesaw St. : Round of 64\n",
            "UC Santa Barbara : Round of 64\n",
            "Northern Kentucky : Round of 64\n",
            "UNC Asheville : Round of 64\n",
            "Texas A&M Corpus Chris : Round of 64\n",
            "Howard  : Round of 64\n",
            "Southeast Missouri St. : Round of 64\n",
            "Fairleigh Dickinson : Round of 68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Jycj8lqZVqu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}